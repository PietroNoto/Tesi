{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAG generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o-mini\",\n",
    "    max_tokens = 256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum, keep the answer concise and DO NOT mention the context and from which documents you take information.\n",
    "    Question: {question}\\n \n",
    "    Context: {context}\\n \n",
    "    Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_EXTRACTOR = \"\"\"\n",
    "    You are tasked to extract useful info from a query provided. \n",
    "    These info are: years, countries, the countries' continent and target (clinic or lab). \n",
    "    If the country is not specified, country and continent will be \"Europe\"; if the target is not specified it will be \"all\". \n",
    "    You must specify the continent of each country in the query.\n",
    "    \n",
    "    Query: {query}\\n\n",
    "    Country:\n",
    "    Continent:\n",
    "    Year:\n",
    "    Target:\n",
    "\"\"\"\n",
    "\n",
    "def metadata_formatter(llm_metadata_output: str):\n",
    "\n",
    "    metadata = {}\n",
    "    fields = llm_metadata_output.replace(\" \", \"\").split(\"\\n\")\n",
    "    metadata[\"country\"] = fields[0].split(\":\")[-1]\n",
    "    metadata[\"continent\"] = fields[1].split(\":\")[-1]\n",
    "    metadata[\"year\"] = fields[2].split(\":\")[-1]\n",
    "    metadata[\"target\"] = fields[3].split(\":\")[-1]\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever-filtered RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def filter_function(doc: Document, metadata):\n",
    "\n",
    "    years = metadata[\"year\"].split(\",\")\n",
    "    countries = metadata[\"country\"].split(\",\")\n",
    "    continents = metadata[\"continent\"].split(\",\")\n",
    "    min_len = min(len(countries), len(continents))\n",
    "    \n",
    "    filter_country = any(countries[i] in doc.metadata[\"keywords\"] or continents[i] in doc.metadata[\"keywords\"] for i in range(min_len))\n",
    "    filter_year = any(f\"{year}\" in doc.metadata[\"keywords\"] for year in years)\n",
    "    filter_target = (f\"{metadata[\"target\"]}\" in doc.metadata[\"keywords\"] or \"all\" in doc.metadata[\"keywords\"] if metadata[\"target\"] == \"lab\" or metadata[\"target\"] == \"clinic\" else True)\n",
    "    \n",
    "    return filter_year and filter_country and filter_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-filtered RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "LLM_DOCUMENT_FILTER = \"\"\"\n",
    "    You are tasked to determine which tag strings are compatible with a provided query.\n",
    "    Each tag string contains: year, country and target:\n",
    "    - year: you have to determine an exact match;\n",
    "    - country:\n",
    "        --one or multiple countries: you have to determine whether at least one of them is compatible;\n",
    "        --continent: you have to determine whether the query's country is compatible.\n",
    "    - target (optional): \"lab\" (laboratories), \"clinic\"; \n",
    "        --\"lab\" is not compatible with \"clinic\" and viceversa;\n",
    "        --if query's target is not specified, you consider correct either \"lab\", \"clinic\" or \"all\" in the tag.\n",
    "    All above conditions must be True for the output to be True.\n",
    "    You have to determine these 3 info from the given query.\n",
    "\n",
    "    Don't include in the output all the breakdown analysis but just a list of boolean values, one for each tag.\n",
    "    Query: {query}\\n\n",
    "    Tag strings: {tags}\n",
    "    Output: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tag_formatter(tag: str):\n",
    "    \n",
    "    tag = tag.replace(\"\\\"\", \"\")\n",
    "    tag = tag.replace(\" \", \"\")\n",
    "    tag = tag.replace(\"country:\", \"\")\n",
    "    tag = tag.replace(\"year:\", \"\")\n",
    "    tag = tag.replace(\"target:\", \"\")\n",
    "    tag = tag.replace(\"lang:non-en,\", \"\")\n",
    "    tag = tag.replace(\",\", \" \")\n",
    "    \n",
    "    return tag\n",
    "    \n",
    "\n",
    "def bitmap_formatter(bitmap_str: str):\n",
    "    \n",
    "    values = bitmap_str[1:-1].split(\",\")\n",
    "    values = [True if \"True\" in val else False for val in values]\n",
    "    return values\n",
    "\n",
    "\n",
    "def filter_context(bitmap_str: str, docs: list[Document]):\n",
    "\n",
    "    bitmap = bitmap_formatter(bitmap_str)\n",
    "    l = min(len(docs), len(bitmap))\n",
    "    bitmap = bitmap[:l]\n",
    "    docs = docs[:l]\n",
    "    relevant_docs = [docs[i] for i in range(l) if bitmap[i]]\n",
    "\n",
    "    return relevant_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG tests runner (simple RAG, RAG with hard filters, RAG with llm filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.docstore.document import Document\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: InMemoryVectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    "    rag_type: str = \"RAG_simple\"\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    \n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    \n",
    "    match rag_type:\n",
    "        \n",
    "        case \"RAG_simple\":\n",
    "            relevant_docs = knowledge_index.similarity_search(query = question, k = num_retrieved_docs)\n",
    "\n",
    "        case \"RAG_filter_retriever\":\n",
    "            llm_metadata_output = llm.invoke(METADATA_EXTRACTOR.format(query = question)).content\n",
    "            metadata = metadata_formatter(llm_metadata_output)\n",
    "            relevant_docs = knowledge_index.similarity_search(query = question, k = num_retrieved_docs, filter = lambda doc: filter_function(doc, metadata))     \n",
    "        \n",
    "        case \"RAG_filter_llm\":\n",
    "            docs = knowledge_index.similarity_search(query = question, k = num_retrieved_docs)\n",
    "            tags = [tag_formatter(doc.metadata[\"keywords\"]) for doc in relevant_docs]\n",
    "            bitmap_str = llm.invoke(LLM_DOCUMENT_FILTER.format(query = question, tags = tags)).content\n",
    "            relevant_docs = filter_context(bitmap_str, docs)\n",
    "\n",
    "        case _: \n",
    "            return\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k = num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    if len(relevant_docs) > num_docs_final:\n",
    "        relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    relevant_docs = [f\"{doc.metadata[\"keywords\"]}\\n{doc.page_content}\" for doc in relevant_docs]\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = SIMPLE_RAG_PROMPT_TEMPLATE.format(question = question, context = context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt).content\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from datasets import Dataset\n",
    "from typing import Optional\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "    k = 4, \n",
    "    k_final = 4,\n",
    "    rag_type: str = \"RAG_simple\"\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, \n",
    "                                                knowledge_index, \n",
    "                                                reranker = reranker,\n",
    "                                                num_retrieved_docs = k,\n",
    "                                                num_docs_final = k_final, \n",
    "                                                rag_type = rag_type)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, indent = 4, ensure_ascii = False)\n",
    "\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'relevant_docs' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m\n\u001b[1;32m     27\u001b[0m knowledge_index \u001b[38;5;241m=\u001b[39m InMemoryVectorStore\u001b[38;5;241m.\u001b[39mload(path \u001b[38;5;241m=\u001b[39m vector_store_path, embedding \u001b[38;5;241m=\u001b[39m embeddings)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# tag1 = \"country: \\\"Italy\\\", year: 2017, target: \\\"lab\\\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# tag2 = \"country: \\\"Spain\\\", year: 2017, target: \\\"clinic\\\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# tag3 = \"country: \\\"Germany\\\", year: 2017, target: \\\"all\\\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# relevant_docs = [relevant_docs[i] for i in range(len(relevant_docs)) if bitmap[i]]\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# bitmap\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m answer, _ \u001b[38;5;241m=\u001b[39m \u001b[43manswer_with_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknowledge_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mknowledge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_retrieved_docs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_docs_final\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrag_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRAG_filter_llm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m answer\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36manswer_with_rag\u001b[0;34m(question, llm, knowledge_index, reranker, num_retrieved_docs, num_docs_final, rag_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m     tags \u001b[38;5;241m=\u001b[39m [tag_formatter(doc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m relevant_docs]\n\u001b[1;32m     34\u001b[0m     bitmap_str \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(LLM_DOCUMENT_FILTER\u001b[38;5;241m.\u001b[39mformat(query \u001b[38;5;241m=\u001b[39m question, tags \u001b[38;5;241m=\u001b[39m tags))\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m---> 35\u001b[0m     relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbitmap_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevant_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m: \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m, in \u001b[0;36mfilter_context\u001b[0;34m(bitmap_str, docs)\u001b[0m\n\u001b[1;32m     47\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs), \u001b[38;5;28mlen\u001b[39m(bitmap))\n\u001b[1;32m     48\u001b[0m bitmap \u001b[38;5;241m=\u001b[39m bitmap[:l]\n\u001b[0;32m---> 49\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[43mrelevant_docs\u001b[49m[:l]\n\u001b[1;32m     50\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m [docs[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(l) \u001b[38;5;28;01mif\u001b[39;00m bitmap[i]]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m relevant_docs\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'relevant_docs' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "all_embeddings = {     \n",
    "    # \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    # \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    # \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    # \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "        model = \"TextEmbedding3LargeDeployment\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        chunk_size = 384\n",
    "    )\n",
    "}\n",
    "\n",
    "question = \"What percentage of non-owners in Spain in 2019 clinics intend to buy a 3D printing unit within the next two years?\"\n",
    "model_name = \"text_embedding_3_large\"\n",
    "\n",
    "vector_store_path = f\"../indexing/models/Text+Images/{model_name}/512_100/512_100_{model_name}\"\n",
    "embeddings = all_embeddings[model_name]\n",
    "knowledge_index = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "\n",
    "# tag1 = \"country: \\\"Italy\\\", year: 2017, target: \\\"lab\\\"\"\n",
    "# tag2 = \"country: \\\"Spain\\\", year: 2017, target: \\\"clinic\\\"\"\n",
    "# tag3 = \"country: \\\"Germany\\\", year: 2017, target: \\\"all\\\"\"\n",
    "# tag4 = \"country: \\\"Europe\\\", year: 2017, target: \\\"all\\\"\"\n",
    "# tag5 = \"country: \\\"Europe\\\", year: 2017, target: \\\"lab\\\"\"\n",
    "# tag6 = \"country: \\\"Italy\\\", year: 2017, target: \\\"clinic\\\"\"\n",
    "\n",
    "\n",
    "# tags = [doc.metadata[\"keywords\"] for doc in relevant_docs]\n",
    "# bitmap = llm.invoke(LLM_DOCUMENT_FILTER.format(query = question, tags = tags)).content\n",
    "# bitmap = bitmap_formatter(bitmap)\n",
    "# relevant_docs = [relevant_docs[i] for i in range(len(relevant_docs)) if bitmap[i]]\n",
    "# bitmap\n",
    "\n",
    "answer, _ = answer_with_rag(question = question, knowledge_index = knowledge_index, llm = llm, num_retrieved_docs = 12, num_docs_final = 4, rag_type = \"RAG_filter_llm\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "all_embeddings = {     \n",
    "    # \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    # \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    # \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    # \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "        model = \"TextEmbedding3LargeDeployment\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        chunk_size = 384\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG...\n",
      "Configuration: model: text_embedding_3_large, chunking: 256_100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5957eae9b7248c7ab1d8968a7c490d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG...\n",
      "Configuration: model: text_embedding_3_large, chunking: 384_100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6e9c753b5749fa8fd2d3f65f06654c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG...\n",
      "Configuration: model: text_embedding_3_large, chunking: 512_100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398f988edb6d42cbbdeda5a7d898f8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def run_tests(chunking_type, \n",
    "              semantic_chunking_type, \n",
    "              model_name, \n",
    "              chunk_size, \n",
    "              chunk_overlap, \n",
    "              eval_dataset,\n",
    "              eval_dataset_name,\n",
    "              generator_name,\n",
    "              rag_type):\n",
    "\n",
    "    if chunking_type == \"page_chunking\":\n",
    "        chunking = chunking_type\n",
    "    elif chunking_type == \"semantic\":\n",
    "        chunking = f\"{chunking_type}_{semantic_chunking_type}\"\n",
    "    else:\n",
    "        chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "    settings_name = f\"chunk:{chunking}_embeddings:{model_name}_reader-model:{generator_name}\"\n",
    "    output_file_name = f\"./output/{generator_name}/Text+Images/{rag_type}/{model_name}/rag_{settings_name}/{eval_dataset_name}\"\n",
    "\n",
    "    if os.path.exists(output_file_name):\n",
    "        return\n",
    "    if not os.path.exists(f\"./output/{generator_name}/Text+Images/{rag_type}/{model_name}/rag_{settings_name}\"):\n",
    "        os.mkdir(f\"./output/{generator_name}/Text+Images/{rag_type}/{model_name}/rag_{settings_name}\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_file_name, \"r\"):\n",
    "            pass\n",
    "    except:\n",
    "        print(\"Running RAG...\")\n",
    "        print(f\"Configuration: model: {model_name}, chunking: {chunking}\")\n",
    "        reranker = None\n",
    "        vector_store_path = f\"../indexing/models/Text+Images/{model_name}/{chunking}/{chunking}_{model_name}\"\n",
    "        embeddings = all_embeddings[model_name]\n",
    "        vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "        run_rag_tests(\n",
    "            eval_dataset = eval_dataset,\n",
    "            llm = llm,\n",
    "            knowledge_index = vector_store,\n",
    "            output_file = output_file_name,\n",
    "            reranker = reranker,\n",
    "            verbose = False,\n",
    "            test_settings = settings_name,\n",
    "            k = 16,\n",
    "            k_final = 4,\n",
    "            rag_type = rag_type\n",
    "        )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "RAG_type = \"RAG_filter_llm\"\n",
    "chunking_types = [\"fixed_number\"]\n",
    "chunk_sizes = [256, 384, 512]\n",
    "chunk_overlaps = [100]\n",
    "semantic_chunking_types = [\"percentile\", \"interquartile\", \"gradient\"]\n",
    "model_names = all_embeddings.keys()\n",
    "eval_dataset_name = \"all_QA_countries&years_brands.json\"\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "with open(f\"../evaluation/dataset/{eval_dataset_name}\", \"r\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "for model_name in model_names:\n",
    "    for chunking_type in chunking_types:\n",
    "\n",
    "        if chunking_type == \"fixed_number\":\n",
    "            for chunk_size in chunk_sizes:\n",
    "                for chunk_overlap in chunk_overlaps:\n",
    "                    run_tests(chunking_type = chunking_type, \n",
    "                            semantic_chunking_type = None, \n",
    "                            model_name = model_name,\n",
    "                            chunk_size = chunk_size,\n",
    "                            chunk_overlap = chunk_overlap,\n",
    "                            eval_dataset = eval_dataset,\n",
    "                            eval_dataset_name = eval_dataset_name,\n",
    "                            generator_name = GENERATOR_MODEL_NAME,\n",
    "                            rag_type = RAG_type)\n",
    "                    \n",
    "        elif chunking_type == \"semantic\":\n",
    "            for semantic_chunking_type in semantic_chunking_types:\n",
    "                run_tests(chunking_type = chunking_type, \n",
    "                        semantic_chunking_type = semantic_chunking_type, \n",
    "                        model_name = model_name,\n",
    "                        chunk_size = None,\n",
    "                        chunk_overlap = None,\n",
    "                        eval_dataset = eval_dataset,\n",
    "                        eval_dataset_name = eval_dataset_name,\n",
    "                        generator_name = GENERATOR_MODEL_NAME,\n",
    "                        rag_type = RAG_type)\n",
    "                \n",
    "        else:\n",
    "            run_tests(chunking_type = chunking_type, \n",
    "                    semantic_chunking_type = None, \n",
    "                    model_name = model_name,\n",
    "                    chunk_size = None,\n",
    "                    chunk_overlap = None,\n",
    "                    eval_dataset = eval_dataset,\n",
    "                    eval_dataset_name = eval_dataset_name,\n",
    "                    generator_name = GENERATOR_MODEL_NAME,\n",
    "                    rag_type = RAG_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
