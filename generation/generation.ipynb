{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAG generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o-mini\",\n",
    "    max_tokens = 256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum, keep the answer concise and DO NOT mention the context and from which documents you take information.\n",
    "    Question: {question}\\n \n",
    "    Context: {context}\\n \n",
    "    Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever-filtered RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "METADATA_EXTRACTOR = \"\"\"\n",
    "    You are tasked to extract useful info from a query provided. \n",
    "    These info are: years, countries, the countries' continent and target (clinic or lab). \n",
    "    If the country is not specified, country and continent will be \"Europe\"; if the target is not specified it will be \"all\". \n",
    "    You must specify the continent of each country in the query.\n",
    "    \n",
    "    Query: {query}\\n\n",
    "    Country:\n",
    "    Continent:\n",
    "    Year:\n",
    "    Target:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def context_filter(llm_metadata_output: str):\n",
    "\n",
    "    metadata = {}\n",
    "    fields = llm_metadata_output.replace(\" \", \"\").split(\"\\n\")\n",
    "    metadata[\"country\"] = fields[0].split(\":\")[-1]\n",
    "    metadata[\"continent\"] = fields[1].split(\":\")[-1]\n",
    "    metadata[\"year\"] = fields[2].split(\":\")[-1]\n",
    "    metadata[\"target\"] = fields[3].split(\":\")[-1]\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def filter_function(doc: Document, metadata):\n",
    "\n",
    "    years = metadata[\"year\"].split(\",\")\n",
    "    countries = metadata[\"country\"].split(\",\")\n",
    "    continents = metadata[\"continent\"].split(\",\")\n",
    "    min_len = min(len(countries), len(continents))\n",
    "    \n",
    "    filter_country = any(countries[i] in doc.metadata[\"keywords\"] or continents[i] in doc.metadata[\"keywords\"] for i in range(min_len))\n",
    "    filter_year = any(f\"{year}\" in doc.metadata[\"keywords\"] for year in years)\n",
    "    filter_target = (f\"{metadata[\"target\"]}\" in doc.metadata[\"keywords\"] or \"all\" in doc.metadata[\"keywords\"] if metadata[\"target\"] == \"lab\" or metadata[\"target\"] == \"clinic\" else True)\n",
    "    \n",
    "    return filter_year and filter_country and filter_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG tests runner (simple RAG, RAG with hard filters, RAG with llm filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.docstore.document import Document\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: InMemoryVectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    "    rag_type: str = \"RAG_simple\"\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    \n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    \n",
    "    match rag_type:\n",
    "        \n",
    "        case \"RAG_simple\":\n",
    "            relevant_docs = knowledge_index.similarity_search(query = question, k = num_retrieved_docs)\n",
    "\n",
    "        case \"RAG_filter_retriever\":\n",
    "            llm_metadata_output = llm.invoke(METADATA_EXTRACTOR.format(query = question)).content\n",
    "            metadata = context_filter(llm_metadata_output)\n",
    "            relevant_docs = knowledge_index.similarity_search(query = question, k = num_retrieved_docs, filter = lambda doc: filter_function(doc, metadata))     \n",
    "        \n",
    "        case _: \n",
    "            return\n",
    "           \n",
    "    relevant_docs = [f\"{doc.metadata[\"keywords\"]}\\n{doc.page_content}\" for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = SIMPLE_RAG_PROMPT_TEMPLATE.format(question = question, context = context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt).content\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from datasets import Dataset\n",
    "from typing import Optional\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "    k = 4, \n",
    "    k_final = 4,\n",
    "    rag_type: str = \"RAG_simple\"\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, \n",
    "                                                knowledge_index, \n",
    "                                                reranker = reranker,\n",
    "                                                num_retrieved_docs = k,\n",
    "                                                num_docs_final = k_final, \n",
    "                                                rag_type = rag_type)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, indent = 4, ensure_ascii = False)\n",
    "\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single run debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "all_embeddings = {     \n",
    "    # \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    # \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    # \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    # \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "        model = \"TextEmbedding3LargeDeployment\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        chunk_size = 384\n",
    "    )\n",
    "}\n",
    "\n",
    "question = \"In 2022, how much was the accessible market potential and the core target in Brazil for CAD-CAM technologies?\"\n",
    "\n",
    "model_name = \"text_embedding_3_large\"\n",
    "\n",
    "vector_store_path = f\"../indexing/models/Text+Images/{model_name}/384_100/384_100_{model_name}\"\n",
    "embeddings = all_embeddings[model_name]\n",
    "knowledge_index = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "answer, _ = answer_with_rag(question=question, knowledge_index=knowledge_index, llm=llm, num_retrieved_docs=4, num_docs_final=4, rag_type=\"RAG_filter_retriever\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "all_embeddings = {     \n",
    "    # \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    # \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    # \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    # \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "        model = \"TextEmbedding3LargeDeployment\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        chunk_size = 384\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def run_tests(chunking_type, \n",
    "              semantic_chunking_type, \n",
    "              model_name, \n",
    "              chunk_size, \n",
    "              chunk_overlap, \n",
    "              eval_dataset,\n",
    "              eval_dataset_name,\n",
    "              generator_name,\n",
    "              rag_type):\n",
    "\n",
    "    if chunking_type == \"page_chunking\":\n",
    "        chunking = chunking_type\n",
    "    elif chunking_type == \"semantic\":\n",
    "        chunking = f\"{chunking_type}_{semantic_chunking_type}\"\n",
    "    else:\n",
    "        chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "    settings_name = f\"chunk:{chunking}_embeddings:{model_name}_reader-model:{generator_name}\"\n",
    "    output_file_name = f\"./output/{generator_name}/Text+Images/{rag_type}/{model_name}/rag_{settings_name}/{eval_dataset_name}\"\n",
    "\n",
    "    if os.path.exists(output_file_name):\n",
    "        return\n",
    "    if not os.path.exists(f\"./output/{generator_name}/Text+Images/{rag_type}/{model_name}/rag_{settings_name}\"):\n",
    "        os.mkdir(f\"./output/{generator_name}/Text+Images/{rag_type}/{model_name}/rag_{settings_name}\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_file_name, \"r\"):\n",
    "            pass\n",
    "    except:\n",
    "        print(\"Running RAG...\")\n",
    "        print(f\"Configuration: model: {model_name}, chunking: {chunking}\")\n",
    "        reranker = None\n",
    "        vector_store_path = f\"../indexing/models/Text+Images/{model_name}/{chunking}/{chunking}_{model_name}\"\n",
    "        embeddings = all_embeddings[model_name]\n",
    "        vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "        run_rag_tests(\n",
    "            eval_dataset = eval_dataset,\n",
    "            llm = llm,\n",
    "            knowledge_index = vector_store,\n",
    "            output_file = output_file_name,\n",
    "            reranker = reranker,\n",
    "            verbose = False,\n",
    "            test_settings = settings_name,\n",
    "            k = 4,\n",
    "            k_final = 4,\n",
    "            rag_type = rag_type\n",
    "        )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "RAG_type = \"RAG_filter_retriever\"\n",
    "chunking_types = [\"fixed_number\"]\n",
    "chunk_sizes = [256, 384, 512]\n",
    "chunk_overlaps = [100]\n",
    "semantic_chunking_types = [\"percentile\", \"interquartile\", \"gradient\"]\n",
    "model_names = all_embeddings.keys()\n",
    "eval_dataset_name = \"all_QA_countries&years_brands.json\"\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "with open(f\"../evaluation/dataset/{eval_dataset_name}\", \"r\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "for model_name in model_names:\n",
    "    for chunking_type in chunking_types:\n",
    "\n",
    "        if chunking_type == \"fixed_number\":\n",
    "            for chunk_size in chunk_sizes:\n",
    "                for chunk_overlap in chunk_overlaps:\n",
    "                    run_tests(chunking_type = chunking_type, \n",
    "                            semantic_chunking_type = None, \n",
    "                            model_name = model_name,\n",
    "                            chunk_size = chunk_size,\n",
    "                            chunk_overlap = chunk_overlap,\n",
    "                            eval_dataset = eval_dataset,\n",
    "                            eval_dataset_name = eval_dataset_name,\n",
    "                            generator_name = GENERATOR_MODEL_NAME,\n",
    "                            rag_type = RAG_type)\n",
    "                    \n",
    "        elif chunking_type == \"semantic\":\n",
    "            for semantic_chunking_type in semantic_chunking_types:\n",
    "                run_tests(chunking_type = chunking_type, \n",
    "                        semantic_chunking_type = semantic_chunking_type, \n",
    "                        model_name = model_name,\n",
    "                        chunk_size = None,\n",
    "                        chunk_overlap = None,\n",
    "                        eval_dataset = eval_dataset,\n",
    "                        eval_dataset_name = eval_dataset_name,\n",
    "                        generator_name = GENERATOR_MODEL_NAME,\n",
    "                        rag_type = RAG_type)\n",
    "                \n",
    "        else:\n",
    "            run_tests(chunking_type = chunking_type, \n",
    "                    semantic_chunking_type = None, \n",
    "                    model_name = model_name,\n",
    "                    chunk_size = None,\n",
    "                    chunk_overlap = None,\n",
    "                    eval_dataset = eval_dataset,\n",
    "                    eval_dataset_name = eval_dataset_name,\n",
    "                    generator_name = GENERATOR_MODEL_NAME,\n",
    "                    rag_type = RAG_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
