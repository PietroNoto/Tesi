{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda activate my_pyenv\n",
    "\n",
    "%pip install --upgrade langchain langchain-community langchain-ollama langchain-huggingface\n",
    "%pip install --upgrade ragas\n",
    "%pip install --upgrade datasetscc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "HF_API_TOKEN = \"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\"\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "\n",
    "files = []\n",
    "for fname in os.listdir(\"../sources\"):\n",
    "    complete_path = os.path.join(\"../sources\", fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        files.append(complete_path)\n",
    "\n",
    "chunk_size = 10_000\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_size/10,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "docs = []\n",
    "page_contents = []\n",
    "giant_docs = []\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file, extract_images = False)\n",
    "    giant_doc = {\"page_content\": \"\", \"metadata\": \"\"}\n",
    "    first = True\n",
    "    async for doc in loader.alazy_load():\n",
    "        if first:\n",
    "            metadata = {k: v for k, v in doc.metadata.items() if k != \"page\"}\n",
    "            giant_doc[\"metadata\"] = metadata\n",
    "        giant_doc[\"page_content\"] += doc.page_content\n",
    "        first = False\n",
    "    giant_docs.append(giant_doc)\n",
    "\n",
    "for gdoc in giant_docs:\n",
    "    page_contents = text_splitter.split_text(gdoc[\"page_content\"])\n",
    "    docs += [{\"metadata\": gdoc[\"metadata\"], \"page_content\": pc} for pc in page_contents]\n",
    "\n",
    "docs = [Document(metadata = doc[\"metadata\"], page_content = doc[\"page_content\"]) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context cleaning for QA generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()\n",
    "\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        if \"non-en\" not in p.metadata[\"keywords\"]:\n",
    "            p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•▪\\-*✔➢●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())\n",
    "\n",
    "\n",
    "remove_non_ASCII(docs)\n",
    "decapitalize_content(docs)\n",
    "remove_bullets(docs)\n",
    "remove_escape(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup agent for questions generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout = None,\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "call_llm(llm_client, \"This is a test context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "N_GENERATIONS = 10\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "with open(\"dataset/all_QA_1.json\", \"w\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/all_QA.json\", \"r\") as f:\n",
    "    outputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(outputs).head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAG generator (LLM model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum, keep the answer concise and DO NOT mention from which documents you take information.\n",
    "    Question: {question}\\n \n",
    "    Context: {context}\\n \n",
    "    Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "READER_MODEL_NAME = \"Zephyr_7B\"\n",
    "HF_API_TOKEN = \"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\"\n",
    "\n",
    "READER_LLM = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=HF_API_TOKEN,\n",
    "    max_new_tokens = 512,\n",
    "    top_k = 30,\n",
    "    temperature = 0.1,\n",
    "    repetition_penalty = 1.03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: InMemoryVectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    \n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "    k = 4, \n",
    "    k_final = 4\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, \n",
    "                                                knowledge_index, \n",
    "                                                reranker = reranker,\n",
    "                                                num_retrieved_docs = k,\n",
    "                                                num_docs_final = k_final)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "all_embeddings = {\n",
    "    \"llama3.2:1b\": OllamaEmbeddings(model = \"llama3.2:1b\"),\n",
    "     \n",
    "    \"llama3.2:3b\": OllamaEmbeddings(model = \"llama3.2:3b\"),\n",
    "     \n",
    "    \"gemma2b\": OllamaEmbeddings(model = \"llama3.2:1b\"),\n",
    "     \n",
    "    \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_tests(chunking_type, semantic_chunking_type, model_name, chunk_size, chunk_overlap, eval_dataset):\n",
    "\n",
    "    if chunking_type == \"page_chunking\":\n",
    "        chunking = chunking_type\n",
    "    elif chunking_type == \"semantic\":\n",
    "        chunking = f\"{chunking_type}_{semantic_chunking_type}\"\n",
    "    else:\n",
    "        chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "    settings_name = f\"chunk:{chunking}_embeddings:{model_name}_reader-model:{READER_MODEL_NAME}\"\n",
    "    output_file_name = f\"./output/rag_{settings_name}/dataset.json\"\n",
    "\n",
    "    if os.path.exists(output_file_name):\n",
    "        return\n",
    "    os.mkdir(f\"./output/rag_{settings_name}\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "        with open(f\"output/rag_{settings_name}/dataset.json\", \"r\") as f:\n",
    "            dataset = json.load(f)\n",
    "    except:\n",
    "        print(\"Running RAG...\")\n",
    "        print(f\"Configuration: model: {model_name}, chunking: {chunking}\")\n",
    "        reranker = None\n",
    "        vector_store_path = f\"../indexing/models/No OCR/{model_name}/{chunking}/{model_name}\"\n",
    "        embeddings = all_embeddings[model_name]\n",
    "        vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "        run_rag_tests(\n",
    "            eval_dataset = eval_dataset,\n",
    "            llm = READER_LLM,\n",
    "            knowledge_index = vector_store,\n",
    "            output_file = output_file_name,\n",
    "            reranker = reranker,\n",
    "            verbose = False,\n",
    "            test_settings = settings_name,\n",
    "            k = 4,\n",
    "            k_final = 4\n",
    "        )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "chunking_types = [\"page_chunking\", \"fixed_number\", \"semantic\"]\n",
    "chunk_sizes = [384, 256]\n",
    "chunk_overlaps = [0, 20, 50, 100]\n",
    "semantic_chunking_types = [\"percentile\", \"standard_deviation\", \"interquartile\", \"gradient\"]\n",
    "model_names = [\"mpnet_base_v2\", \"minilm_l6\", \"minilm_l12\"]\n",
    "READER_MODEL_NAME = \"Zephyr_7B\"\n",
    "\n",
    "with open(\"dataset/all_QA.json\", \"r\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "for model_name in model_names:\n",
    "    for chunking_type in chunking_types:\n",
    "\n",
    "        if chunking_type == \"fixed_number\":\n",
    "            for chunk_size in chunk_sizes:\n",
    "                for chunk_overlap in chunk_overlaps:\n",
    "                    run_tests(chunking_type = chunking_type, \n",
    "                              semantic_chunking_type = None, \n",
    "                              model_name = model_name,\n",
    "                              chunk_size = chunk_size,\n",
    "                              chunk_overlap = chunk_overlap,\n",
    "                              eval_dataset = eval_dataset)\n",
    "                    \n",
    "        elif chunking_type == \"semantic\":\n",
    "            for semantic_chunking_type in semantic_chunking_types:\n",
    "                run_tests(chunking_type = chunking_type, \n",
    "                              semantic_chunking_type = semantic_chunking_type, \n",
    "                              model_name = model_name,\n",
    "                              chunk_size = None,\n",
    "                              chunk_overlap = None,\n",
    "                              eval_dataset = eval_dataset)\n",
    "                \n",
    "        else:\n",
    "            run_tests(chunking_type = chunking_type, \n",
    "                              semantic_chunking_type = None, \n",
    "                              model_name = model_name,\n",
    "                              chunk_size = None,\n",
    "                              chunk_overlap = None,\n",
    "                              eval_dataset = eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.run_config import RunConfig\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from ragas.metrics import context_precision, answer_relevancy, faithfulness, context_recall\n",
    "from ragas import EvaluationDataset\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "HF_API_TOKEN = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "run_config = RunConfig(timeout = 600, max_wait = 600)\n",
    "\n",
    "chunking_type = \"page_chunking\"\n",
    "chunk_size = 256\n",
    "chunk_overlap = 100\n",
    "chunking = chunking_type if chunking_type != \"fixed_number\" else f\"{chunk_size}_{chunk_overlap}\"\n",
    "model_name = \"mpnet_base_v2\"\n",
    "READER_MODEL_NAME = \"Zephyr_7B\"\n",
    "\n",
    "settings_name = f\"chunk:{chunking}_embeddings:{model_name}_reader-model:{READER_MODEL_NAME}\"\n",
    "\n",
    "with open(f\"output/rag_{settings_name}.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "dataset = dataset[:1]\n",
    "d = {\n",
    "    \"user_input\": [entry[\"question\"] for entry in dataset],\n",
    "    \"retrieved_contexts\": [entry[\"retrieved_docs\"] for entry in dataset],\n",
    "    \"response\": [entry[\"generated_answer\"] for entry in dataset],\n",
    "    \"reference\": [entry[\"true_answer\"] for entry in dataset],\n",
    "}\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(Dataset.from_dict(d))\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id = 'meta-llama/Llama-3.2-3B',\n",
    "#     huggingfacehub_api_token = HF_API_TOKEN,\n",
    "# )\n",
    "llm = ChatOllama(model = \"llama3.2:1b\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "# embeddings = OllamaEmbeddings(model = \"llama3.2:1b\")\n",
    "\n",
    "results = evaluate(dataset = eval_dataset, \n",
    "                   metrics = [context_precision], \n",
    "                   llm = llm, \n",
    "                   embeddings = embeddings,\n",
    "                   run_config = run_config)\n",
    "\n",
    "results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
