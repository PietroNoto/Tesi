{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from ragas.metrics import context_precision, answer_relevancy, faithfulness, context_recall, answer_correctness\n",
    "from ragas.run_config import RunConfig\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o\"\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model=\"TextEmbedding3LargeDeployment\",\n",
    "    api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "for model_name in [\"multilingual\", \"text_embedding_3_large\"]:\n",
    "    for chunking_type in [\"page_chunking\", \"fixed_number\"]:\n",
    "        for chunk_size in [256, 384]:\n",
    "            for chunk_overlap in [0, 20, 50, 100]:\n",
    "\n",
    "                if chunking_type == \"page_chunking\":\n",
    "                    chunking = chunking_type\n",
    "                elif chunking_type == \"semantic\":\n",
    "                    chunking = f\"{chunking_type}_{semantic_chunking_type}\"\n",
    "                else:\n",
    "                    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "                settings_name = f\"rag_chunk:{chunking}_embeddings:{model_name}_reader-model:{GENERATOR_MODEL_NAME}\"\n",
    "                folder = f\"output/{GENERATOR_MODEL_NAME}/Text+Images/RAG_filter_hard/{settings_name}\"\n",
    "\n",
    "                with open(f\"{folder}/dataset_countries&years_brands.json\", \"r\") as f:\n",
    "                    dataset = json.load(f)\n",
    "\n",
    "                # Take a subset\n",
    "                dataset = dataset[:15]\n",
    "\n",
    "                d = {\n",
    "                    \"question\": [entry[\"question\"] for entry in dataset],\n",
    "                    \"contexts\": [entry[\"retrieved_docs\"] for entry in dataset],\n",
    "                    \"answer\": [entry[\"generated_answer\"] for entry in dataset],\n",
    "                    \"ground_truth\": [entry[\"true_answer\"] for entry in dataset],\n",
    "                }\n",
    "\n",
    "                eval_dataset = Dataset.from_dict(d)\n",
    "\n",
    "                if not os.path.exists(f\"{folder}/results_countries&years_brands.json\"):\n",
    "                    run_config = RunConfig(timeout = 6000, max_retries = 20, max_wait = 50, log_tenacity = False)\n",
    "                    print(settings_name)\n",
    "                    results = evaluate(dataset = eval_dataset, \n",
    "                                    metrics = [context_precision, faithfulness, answer_relevancy, context_recall, answer_correctness], \n",
    "                                    llm = llm, \n",
    "                                    embeddings = embeddings,\n",
    "                                    run_config = run_config)\n",
    "                    \n",
    "                    results.to_pandas().to_json(f\"{folder}/results_countries&years_brands.json\", indent = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS results processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from statistics import mean, variance\n",
    "\n",
    "\n",
    "RAG_type = \"RAG_filter_retriever\"\n",
    "chunking_type = \"fixed_number\"\n",
    "chunk_size = 256\n",
    "chunk_overlap = 100\n",
    "model_name = \"text_embedding_3_large\"\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "  chunking = chunking_type\n",
    "else:\n",
    "    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "settings_name = f\"output/{GENERATOR_MODEL_NAME}/Text+Images/{RAG_type}/rag_chunk:{chunking}_embeddings:{model_name}_reader-model:{GENERATOR_MODEL_NAME}\"\n",
    "path1 = f\"{settings_name}/results_easy.json\"\n",
    "path2 = f\"{settings_name}/results_years_countries.json\"\n",
    "path3 = f\"{settings_name}/results_countries&years_brands.json\"\n",
    "\n",
    "dataset1 = Dataset.from_json(path1)\n",
    "dataset2 = Dataset.from_json(path2)\n",
    "dataset3 = Dataset.from_json(path3)\n",
    "\n",
    "metrics1 = {}\n",
    "metrics2 = {}\n",
    "metrics3 = {}\n",
    "\n",
    "print(settings_name + \"\\n\")\n",
    "for metric in [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\", \"answer_correctness\"]:\n",
    "\n",
    "    for d in dataset1[metric]:\n",
    "        metrics1[metric] = {\"mean\": mean([ d[entry] if d[entry] != None else 0.0 for entry in d ]),\n",
    "              \"variance\": variance([ d[entry] if d[entry] != None else 0.0 for entry in d ]) }\n",
    "    for d in dataset2[metric]:\n",
    "        metrics2[metric] = {\"mean\": mean([ d[entry] if d[entry] != None else 0.0 for entry in d ]),\n",
    "              \"variance\": variance([ d[entry] if d[entry] != None else 0.0 for entry in d ]) }\n",
    "    for d in dataset3[metric]:\n",
    "        metrics3[metric] = {\"mean\": mean([ d[entry] if d[entry] != None else 0.0 for entry in d ]),\n",
    "              \"variance\": variance([ d[entry] if d[entry] != None else 0.0 for entry in d ]) }\n",
    "\n",
    "    (m1, v1) = metrics1[metric][\"mean\"], metrics1[metric][\"variance\"]\n",
    "    # (m2, v2) = metrics2[metric][\"mean\"], metrics2[metric][\"variance\"]\n",
    "    # (m3, v3) = metrics3[metric][\"mean\"], metrics3[metric][\"variance\"]\n",
    "    # print(f\"{metric}:\\n{mean([m1, m2, m3]):.3f}, {mean([v1, v2, v3]):.3f}\")\n",
    "    print(f\"{m1:.3f}, \")\n",
    "\n",
    "# all_answers = list(dataset1[\"response\"][0].values()) + list(dataset2[\"response\"][0].values()) + list(dataset3[\"response\"][0].values())\n",
    "# idks = len(list(filter(lambda ans: \"I don't know\" in ans, all_answers)))\n",
    "# print(\"\\n\" + \"I don't know:\\n\" + str(idks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect post-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "path = f\"{folder}/results.json\"\n",
    "dataset = Dataset.from_json(path)\n",
    "\n",
    "print(path)\n",
    "cc\n",
    "key = \"2\"\n",
    "# print(f\"{dataset[\"user_input\"][0][key]}\")\n",
    "# print(f\"{dataset[\"reference\"][0][key]}\")\n",
    "print(f\"{dataset[\"response\"][0][key]}\")\n",
    "print(f\"{dataset[\"context_precision\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"faithfulness\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"answer_relevancy\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"context_recall\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"answer_correctness\"][0][key]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
