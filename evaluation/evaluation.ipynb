{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from ragas.metrics import context_precision, answer_relevancy, faithfulness, context_recall, answer_correctness\n",
    "from ragas.run_config import RunConfig\n",
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o\"\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model=\"TextEmbedding3LargeDeployment\",\n",
    "    api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "ragas_metrics = [context_precision, faithfulness, answer_relevancy, context_recall, answer_correctness]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "dataset_type = \"countries&years_brands\"\n",
    "rag_type = \"RAG_filter_retriever\"\n",
    "\n",
    "input_file = f\"all_QA_{dataset_type}.json\"\n",
    "output_file = f\"results_{dataset_type}.json\"\n",
    "\n",
    "for model_name in [\"text_embedding_3_large\"]:\n",
    "    for chunking_type in [\"fixed_number\"]:\n",
    "        for chunk_size in [256, 384, 512]:\n",
    "            for chunk_overlap in [100]:\n",
    "\n",
    "                if chunking_type == \"page_chunking\":\n",
    "                    chunking = chunking_type\n",
    "                elif chunking_type == \"fixed_number\":\n",
    "                    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "                settings_name = f\"rag_chunk:{chunking}_embeddings:{model_name}_reader-model:{GENERATOR_MODEL_NAME}\"\n",
    "                folder = f\"output/{GENERATOR_MODEL_NAME}/Text+Images/{rag_type}/{model_name}/{settings_name}\"\n",
    "                input_folder = \"../generation/\" + folder\n",
    "                run_config = RunConfig(timeout = 1200, max_retries = 20, max_wait = 50, log_tenacity = False)\n",
    "\n",
    "                if not os.path.exists(f\"{folder}\"):\n",
    "                    os.mkdir(f\"{folder}\")\n",
    "\n",
    "                with open(f\"{input_folder}/{input_file}\", \"r\") as f:\n",
    "                    dataset = json.load(f)\n",
    "\n",
    "                if not os.path.exists(f\"{folder}/{output_file}\"):   \n",
    "                    all_results = []\n",
    "                    print(settings_name)\n",
    "\n",
    "                    with open(f\"{folder}/{output_file}\", \"w\") as f:\n",
    "                        for entry in tqdm(dataset):\n",
    "    \n",
    "                            eval_dataset = Dataset.from_dict({\n",
    "                                \"question\": [entry[\"question\"]],\n",
    "                                \"contexts\": [entry[\"retrieved_docs\"]],\n",
    "                                \"answer\": [entry[\"generated_answer\"]],\n",
    "                                \"ground_truth\": [entry[\"true_answer\"]],\n",
    "                            })\n",
    "                            results = evaluate(dataset = eval_dataset, \n",
    "                                            metrics = ragas_metrics, \n",
    "                                            llm = llm, \n",
    "                                            embeddings = embeddings,\n",
    "                                            run_config = run_config,\n",
    "                                            show_progress = False)\n",
    "                            \n",
    "                            results = results.to_pandas()\n",
    "                        \n",
    "                            all_results.append({\n",
    "                                \"question\": results.loc[0, \"user_input\"],\n",
    "                                \"retrieved_docs\": results.loc[0, \"retrieved_contexts\"],\n",
    "                                \"generated_answer\": results.loc[0, \"response\"],\n",
    "                                \"true_answer\": results.loc[0, \"reference\"],\n",
    "                                \"cp\": results.loc[0, \"context_precision\"],\n",
    "                                \"f\": results.loc[0, \"faithfulness\"],\n",
    "                                \"ar\": results.loc[0, \"answer_relevancy\"],\n",
    "                                \"cr\": results.loc[0, \"context_recall\"],\n",
    "                                \"ac\": results.loc[0, \"answer_correctness\"]\n",
    "                            })\n",
    "\n",
    "                        json.dump(all_results, f, indent = 4, ensure_ascii = False)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS results processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, stdev\n",
    "from math import isnan\n",
    "\n",
    "\n",
    "RAG_type = \"RAG_filter_retriever\"\n",
    "chunking_type = \"fixed_number\"\n",
    "chunk_size = 512\n",
    "chunk_overlap = 100\n",
    "model_name = \"text_embedding_3_large\"\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "  chunking = chunking_type\n",
    "else:\n",
    "    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "settings_name = f\"output/{GENERATOR_MODEL_NAME}/Text+Images/{RAG_type}/{model_name}/rag_chunk:{chunking}_embeddings:{model_name}_reader-model:{GENERATOR_MODEL_NAME}\"\n",
    "path1 = f\"{settings_name}/results_easy.json\"\n",
    "path2 = f\"{settings_name}/results_years_countries.json\"\n",
    "path3 = f\"{settings_name}/results_countries&years_brands.json\"\n",
    "\n",
    "d1, d2, d3 = open(path1, \"r\"), open(path2, \"r\"), open(path3, \"r\")\n",
    "dataset1, dataset2, dataset3 = json.load(d1), json.load(d2), json.load(d3)\n",
    "d1.close(), d2.close(), d3.close()\n",
    "\n",
    "metrics1 = {}\n",
    "metrics2 = {}\n",
    "metrics3 = {}\n",
    "\n",
    "print(settings_name + \"\\n\")\n",
    "for metric in [\"cp\",\"f\", \"ar\", \"cr\", \"ac\"]:\n",
    "    \n",
    "    metrics1[metric] = {\n",
    "        \"mean\": mean([ entry[metric] if not isnan(entry[metric]) else 0.0 for entry in dataset1 ]),\n",
    "        \"stdev\": stdev([ entry[metric] if not isnan(entry[metric]) else 0.0 for entry in dataset1 ]) \n",
    "    }\n",
    "    metrics2[metric] = {\n",
    "        \"mean\": mean([ entry[metric] if not isnan(entry[metric]) else 0.0 for entry in dataset2 ]),\n",
    "        \"stdev\": stdev([ entry[metric] if not isnan(entry[metric]) else 0.0 for entry in dataset2 ]) \n",
    "    }\n",
    "    metrics3[metric] = {\n",
    "        \"mean\": mean([ entry[metric] if not isnan(entry[metric]) else 0.0 for entry in dataset3 ]),\n",
    "        \"stdev\": stdev([ entry[metric] if not isnan(entry[metric]) else 0.0 for entry in dataset3 ]) \n",
    "    }\n",
    "    \n",
    "    (m1, v1) = metrics1[metric][\"mean\"], metrics1[metric][\"stdev\"]\n",
    "    (m2, v2) = metrics2[metric][\"mean\"], metrics2[metric][\"stdev\"]\n",
    "    (m3, v3) = metrics3[metric][\"mean\"], metrics3[metric][\"stdev\"]\n",
    "\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"{mean([m1, m2, m3]):.3f}, {mean([v1, v2, v3]):.3f}\")\n",
    "\n",
    "filter_f = lambda ans: \"I don't know\" in ans\n",
    "print(\"\\nI don't know:\")\n",
    "for i, dataset in enumerate([dataset1, dataset2, dataset3]):\n",
    "    idks = len(list(filter(filter_f, [qa[\"generated_answer\"] for qa in dataset])))\n",
    "    print(f\"Dataset {i + 1}: {idks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3[11][\"generated_answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute intersection and union between retrieved context and reference context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection:\n",
      "140.3, 41.3\n",
      "Union:\n",
      "10.13, 0.75\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "from statistics import mean, stdev\n",
    "\n",
    "\n",
    "dataset_type = \"countries&years_brands\"\n",
    "rag_type = \"RAG_filter_retriever\"\n",
    "model_name = \"text_embedding_3_large\"\n",
    "chunking_type = \"fixed_number\"\n",
    "chunk_size = 512\n",
    "chunk_overlap = 100\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "  chunking = chunking_type\n",
    "elif chunking_type == \"fixed_number\":\n",
    "    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "reference_file = f\"dataset/all_QA_{dataset_type}.json\"\n",
    "base_path = f\"../generation/output/GPT_4o_mini/Text+Images/{rag_type}/{model_name}\"\n",
    "settings = f\"rag_chunk:{chunking}_embeddings:{model_name}_reader-model:GPT_4o_mini\"\n",
    "generation_file = f\"{base_path}/{settings}/all_QA_{dataset_type}.json\"\n",
    "res_file = f\"{base_path.replace(\"generation\", \"evaluation\")}/{settings}/results_{dataset_type}.json\"\n",
    "\n",
    "f1 = open(reference_file, \"r\")\n",
    "f2 = open(generation_file, \"r\")\n",
    "rf = json.load(f1)\n",
    "gf = json.load(f2)\n",
    "\n",
    "intersections = []\n",
    "unions = []\n",
    "\n",
    "for item in list(zip(rf, gf)):\n",
    "    ref_context = item[0][\"context\"]\n",
    "    gen_context = \"\".join(item[1][\"retrieved_docs\"])\n",
    "\n",
    "    a = ref_context.split()\n",
    "    b = gen_context.split()\n",
    "    seq_matcher = SequenceMatcher(None, a, b)\n",
    "    lcs = seq_matcher.find_longest_match(0, len(a), 0, len(b))\n",
    "\n",
    "    intersection = lcs.size\n",
    "    union = (len(a) + len(b) - intersection) / 256\n",
    "    intersections.append(intersection)\n",
    "    unions.append(union)\n",
    "    jaccards = [intersections[i]/unions[i] for i in range(len(intersections))]\n",
    "\n",
    "with open(res_file, \"r\") as f3:\n",
    "    resf = json.load(f3)\n",
    "    # resf[\"intersection\"] = {str(i): intersections[i] for i in range(len(intersections))}   \n",
    "    # resf[\"union\"] = {str(i): unions[i] for i in range(len(unions))} \n",
    "    for i, entry in enumerate(resf):\n",
    "        entry[\"intersection\"] = intersections[i]\n",
    "        entry[\"union\"] = unions[i]\n",
    "        entry[\"jaccard\"] = jaccards[i]\n",
    "\n",
    "# resf = {\"intersection\": intersections, \"union\": unions, \"jaccard\": jaccards}\n",
    "with open(res_file, \"w\") as f3:\n",
    "    json.dump(resf, f3, indent = 4, ensure_ascii = False)\n",
    "\n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "(m_i, v_i) = mean(intersections), stdev(intersections)\n",
    "(m_u, v_u) = mean(unions), stdev(unions)\n",
    "\n",
    "print(f\"Intersection:\\n{m_i:.1f}, {v_i:.1f}\")\n",
    "print(f\"Union:\\n{m_u:.2f}, {v_u:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detector between RAG types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "rag_type_1 = \"RAG_simple\"\n",
    "rag_type_2 = \"RAG_filter_retriever\"\n",
    "dataset_name = \"results_countries&years_brands.json\"\n",
    "model_name = \"text_embedding_3_large\"\n",
    "chunk_size = 512\n",
    "chunk_overlap = 100\n",
    "\n",
    "settings_name_1 = f\"output/GPT_4o_mini/Text+Images/{rag_type_1}/{model_name}/rag_chunk:{chunk_size}_{chunk_overlap}_embeddings:{model_name}_reader-model:GPT_4o_mini\"\n",
    "settings_name_2 = f\"output/GPT_4o_mini/Text+Images/{rag_type_2}/{model_name}/rag_chunk:{chunk_size}_{chunk_overlap}_embeddings:{model_name}_reader-model:GPT_4o_mini\"\n",
    "\n",
    "path1 = f\"{settings_name_1}/{dataset_name}\"\n",
    "path2 = f\"{settings_name_2}/{dataset_name}\"\n",
    "dataset1 = Dataset.from_json(path1)\n",
    "dataset2 = Dataset.from_json(path2)\n",
    "\n",
    "# 2 should be advanced RAG and 1 simple RAG\n",
    "ints_simple = list(dataset1[\"intersection\"][0].values())\n",
    "ints_advanced = list(dataset2[\"intersection\"])\n",
    "\n",
    "anomalies = [i for i, val in enumerate(ints_advanced) if val < ints_simple[i]]\n",
    "anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15: 97 vs 117\n"
     ]
    }
   ],
   "source": [
    "for a in anomalies:\n",
    "    print(f\"{a}: {ints_advanced[a]} vs {ints_simple[a]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
