{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from ragas.metrics import context_precision, answer_relevancy, faithfulness, context_recall, answer_correctness\n",
    "from ragas.run_config import RunConfig\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o\"\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model=\"TextEmbedding3LargeDeployment\",\n",
    "    api_version=\"2023-05-15\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "dataset_type = \"easy\"\n",
    "rag_type = \"RAG_filter_retriever\"\n",
    "\n",
    "input_file = f\"all_QA_{dataset_type}.json\"\n",
    "output_file = f\"results_{dataset_type}.json\"\n",
    "\n",
    "for model_name in [\"text_embedding_3_large\"]:\n",
    "    for chunking_type in [\"fixed_number\"]:\n",
    "        for chunk_size in [256, 384, 512]:\n",
    "            for chunk_overlap in [100]:\n",
    "\n",
    "                if chunking_type == \"page_chunking\":\n",
    "                    chunking = chunking_type\n",
    "                elif chunking_type == \"fixed_number\":\n",
    "                    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "                settings_name = f\"rag_chunk:{chunking}_embeddings:{model_name}_reader-model:{GENERATOR_MODEL_NAME}\"\n",
    "                folder = f\"output/{GENERATOR_MODEL_NAME}/Text+Images/{rag_type}/{model_name}/{settings_name}\"\n",
    "                input_folder = \"../generation/\" + folder\n",
    "\n",
    "                with open(f\"{input_folder}/{input_file}\", \"r\") as f:\n",
    "                    dataset = json.load(f)\n",
    "\n",
    "                # Take a subset\n",
    "                # dataset = dataset[:15]\n",
    "\n",
    "                d = {\n",
    "                    \"question\": [entry[\"question\"] for entry in dataset],\n",
    "                    \"contexts\": [entry[\"retrieved_docs\"] for entry in dataset],\n",
    "                    \"answer\": [entry[\"generated_answer\"] for entry in dataset],\n",
    "                    \"ground_truth\": [entry[\"true_answer\"] for entry in dataset],\n",
    "                }\n",
    "\n",
    "                eval_dataset = Dataset.from_dict(d)\n",
    "                \n",
    "                if not os.path.exists(f\"{folder}\"):\n",
    "                    os.mkdir(f\"{folder}\")\n",
    "                \n",
    "                if not os.path.exists(f\"{folder}/{output_file}\"):\n",
    "                    run_config = RunConfig(timeout = 50_000, max_retries = 20, max_wait = 50, log_tenacity = False)\n",
    "                    print(settings_name)\n",
    "                    results = evaluate(dataset = eval_dataset, \n",
    "                                    metrics = [context_precision, faithfulness, answer_relevancy, context_recall, answer_correctness], \n",
    "                                    llm = llm, \n",
    "                                    embeddings = embeddings,\n",
    "                                    run_config = run_config)\n",
    "                    \n",
    "                    results.to_pandas().to_json(f\"{folder}/{output_file}\", indent = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS results processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from statistics import mean, stdev\n",
    "\n",
    "\n",
    "RAG_type = \"RAG_filter_retriever\"\n",
    "chunking_type = \"fixed_number\"\n",
    "chunk_size = 512\n",
    "chunk_overlap = 100\n",
    "model_name = \"text_embedding_3_large\"\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "  chunking = chunking_type\n",
    "else:\n",
    "    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "settings_name = f\"output/{GENERATOR_MODEL_NAME}/Text+Images/{RAG_type}/{model_name}/rag_chunk:{chunking}_embeddings:{model_name}_reader-model:{GENERATOR_MODEL_NAME}\"\n",
    "path1 = f\"{settings_name}/results_easy.json\"\n",
    "path2 = f\"{settings_name}/results_years_countries.json\"\n",
    "path3 = f\"{settings_name}/results_countries&years_brands.json\"\n",
    "\n",
    "dataset1 = Dataset.from_json(path1)\n",
    "dataset2 = Dataset.from_json(path2)\n",
    "dataset3 = Dataset.from_json(path3)\n",
    "\n",
    "metrics1 = {}\n",
    "metrics2 = {}\n",
    "metrics3 = {}\n",
    "\n",
    "print(settings_name + \"\\n\")\n",
    "for metric in [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\", \"answer_correctness\"]:\n",
    "      \n",
    "    # for d in dataset1[metric]:\n",
    "    #     metrics1[metric] = {\"mean\": mean([ d[entry] if d[entry] != None else 0.0 for entry in d ]),\n",
    "    #     \t\"stdev\": stdev([ d[entry] if d[entry] != None else 0.0 for entry in d ]) }\n",
    "\n",
    "    metrics1[metric] = {\n",
    "        \"mean\": mean([ val if val != None else 0.0 for val in dataset1[metric] ]),\n",
    "        \"stdev\": stdev([ val if val != None else 0.0 for val in dataset1[metric] ]) \n",
    "    }\n",
    "    \n",
    "    for d in dataset2[metric]:\n",
    "        metrics2[metric] = {\"mean\": mean([ d[entry] if d[entry] != None else 0.0 for entry in d ]),\n",
    "              \"stdev\": stdev([ d[entry] if d[entry] != None else 0.0 for entry in d ]) }\n",
    "        \n",
    "    for d in dataset3[metric]:\n",
    "        metrics3[metric] = {\"mean\": mean([ d[entry] if d[entry] != None else 0.0 for entry in d ]),\n",
    "              \"stdev\": stdev([ d[entry] if d[entry] != None else 0.0 for entry in d ]) }\n",
    "\t\n",
    "    (m1, v1) = metrics1[metric][\"mean\"], metrics1[metric][\"stdev\"]\n",
    "    (m2, v2) = metrics2[metric][\"mean\"], metrics2[metric][\"stdev\"]\n",
    "    (m3, v3) = metrics3[metric][\"mean\"], metrics3[metric][\"stdev\"]\n",
    "\n",
    "    print(f\"{metric}:\")\n",
    "    # print(f\"{m1:.3f}, {v1:.3f}\\n{m2:.3f}, {v2:.3f}\\n{m3:.3f}, {v3:.3f}\")\n",
    "    print(f\"{mean([m1, m2, m3]):.3f}, {mean([v1, v2, v3]):.3f}\")\n",
    "\n",
    "filter_f = lambda ans: \"I don't know\" in ans\n",
    "idks1 = len(list(filter(lambda ans: \"I don't know\" in ans, dataset1[\"response\"])))\n",
    "idks2 = len(list(filter(lambda ans: \"I don't know\" in ans, dataset2[\"response\"][0].values())))\n",
    "idks3 = len(list(filter(lambda ans: \"I don't know\" in ans, dataset3[\"response\"][0].values())))\n",
    "\n",
    "print(\"\\n\" + \"I don't know:\")\n",
    "# print(f\"{idks1}\\n{idks2}\\n{idks3}\")\n",
    "print(f\"{idks1 + idks2 + idks3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect post-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "path = f\"{folder}/results.json\"\n",
    "dataset = Dataset.from_json(path)\n",
    "\n",
    "print(path)\n",
    "key = \"2\"\n",
    "# print(f\"{dataset[\"user_input\"][0][key]}\")\n",
    "# print(f\"{dataset[\"reference\"][0][key]}\")\n",
    "print(f\"{dataset[\"response\"][0][key]}\")\n",
    "print(f\"{dataset[\"context_precision\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"faithfulness\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"answer_relevancy\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"context_recall\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"answer_correctness\"][0][key]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute intersection and union between retrieved context and reference context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection:\n",
      "79.1, 80.8\n",
      "Union:\n",
      "3.67, 0.54\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import json\n",
    "from statistics import mean, stdev\n",
    "\n",
    "\n",
    "dataset_type = \"easy\"\n",
    "rag_type = \"RAG_filter_retriever\"\n",
    "model_name = \"text_embedding_3_large\"\n",
    "chunking_type = \"fixed_number\"\n",
    "chunk_size = 256\n",
    "chunk_overlap = 100\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "  chunking = chunking_type\n",
    "elif chunking_type == \"fixed_number\":\n",
    "    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "reference_file = f\"dataset/all_QA_{dataset_type}.json\"\n",
    "base_path = f\"../generation/output/GPT_4o_mini/Text+Images/{rag_type}/{model_name}\"\n",
    "settings = f\"rag_chunk:{chunking}_embeddings:{model_name}_reader-model:GPT_4o_mini\"\n",
    "generation_file = f\"{base_path}/{settings}/dataset_{dataset_type}.json\"\n",
    "\n",
    "f1 = open(reference_file, \"r\")\n",
    "f2 = open(generation_file, \"r\")\n",
    "rf = json.load(f1)\n",
    "gf = json.load(f2)\n",
    "\n",
    "intersections = []\n",
    "unions = []\n",
    "\n",
    "for item in list(zip(rf, gf)):\n",
    "    ref_context = item[0][\"context\"]\n",
    "    gen_context = \"\".join(item[1][\"retrieved_docs\"])\n",
    "\n",
    "    a = ref_context.split()\n",
    "    b = gen_context.split()\n",
    "    seq_matcher = SequenceMatcher(None, a, b)\n",
    "    lcs = seq_matcher.find_longest_match(0, len(a), 0, len(b))\n",
    "\n",
    "    intersection = lcs.size\n",
    "    union = (len(a) + len(b) - intersection) / 256\n",
    "    intersections.append(intersection)\n",
    "    unions.append(union)\n",
    "    \n",
    "f1.close()\n",
    "f2.close()\n",
    "\n",
    "(m_i, v_i) = mean(intersections), stdev(intersections)\n",
    "(m_u, v_u) = mean(unions), stdev(unions)\n",
    "\n",
    "print(f\"Intersection:\\n{m_i:.1f}, {v_i:.1f}\")\n",
    "print(f\"Union:\\n{m_u:.2f}, {v_u:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
