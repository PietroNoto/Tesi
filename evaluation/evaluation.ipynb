{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAG generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o-mini\",\n",
    "    max_tokens = 256\n",
    ")\n",
    "\n",
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum, keep the answer concise and DO NOT mention from which documents you take information.\n",
    "    Question: {question}\\n \n",
    "    Context: {context}\\n \n",
    "    Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.docstore.document import Document\n",
    "from typing import Optional, List, Tuple\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "\n",
    "\n",
    "@retry(wait = wait_random_exponential(min = 5, max = 60), stop = stop_after_attempt(10))\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: InMemoryVectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    \n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question = question, context = context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt).content\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from datasets import Dataset\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "    k = 4, \n",
    "    k_final = 4\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, \n",
    "                                                knowledge_index, \n",
    "                                                reranker = reranker,\n",
    "                                                num_retrieved_docs = k,\n",
    "                                                num_docs_final = k_final)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)\n",
    "\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "all_embeddings = {     \n",
    "    \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "        model = \"TextEmbedding3LargeDeployment\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        chunk_size = 384\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def run_tests(chunking_type, \n",
    "              semantic_chunking_type, \n",
    "              model_name, chunk_size, \n",
    "              chunk_overlap, \n",
    "              eval_dataset, \n",
    "              generator_name):\n",
    "\n",
    "    # if chunking_type != \"fixed_number\" or chunk_size != 384 or chunk_overlap != 100 or model_name != \"mpnet_base_v2\":\n",
    "    #     return\n",
    "    \n",
    "    if chunking_type == \"page_chunking\":\n",
    "        chunking = chunking_type\n",
    "    elif chunking_type == \"semantic\":\n",
    "        chunking = f\"{chunking_type}_{semantic_chunking_type}\"\n",
    "    else:\n",
    "        chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "    settings_name = f\"chunk:{chunking}_embeddings:{model_name}_reader-model:{generator_name}\"\n",
    "    output_file_name = f\"./output/{generator_name}/rag_{settings_name}/dataset.json\"\n",
    "\n",
    "    if os.path.exists(output_file_name):\n",
    "        return\n",
    "    os.mkdir(f\"./output/{generator_name}/rag_{settings_name}\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "        with open(f\"output/{generator_name}/rag_{settings_name}/dataset.json\", \"r\") as f:\n",
    "            dataset = json.load(f)\n",
    "    except:\n",
    "        print(\"Running RAG...\")\n",
    "        print(f\"Configuration: model: {model_name}, chunking: {chunking}\")\n",
    "        reranker = None\n",
    "        vector_store_path = f\"../indexing/models/No OCR/{model_name}/{chunking}/{model_name}\"\n",
    "        embeddings = all_embeddings[model_name]\n",
    "        vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "        run_rag_tests(\n",
    "            eval_dataset = eval_dataset,\n",
    "            llm = llm,\n",
    "            knowledge_index = vector_store,\n",
    "            output_file = output_file_name,\n",
    "            reranker = reranker,\n",
    "            verbose = False,\n",
    "            test_settings = settings_name,\n",
    "            k = 4,\n",
    "            k_final = 4\n",
    "        )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "chunking_types = [\"page_chunking\", \"fixed_number\"]\n",
    "chunk_sizes = [384, 256]\n",
    "chunk_overlaps = [0, 20, 50, 100]\n",
    "semantic_chunking_types = [\"percentile\", \"interquartile\", \"gradient\"]\n",
    "model_names = all_embeddings.keys()\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "with open(\"dataset/all_QA.json\", \"r\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "for model_name in model_names:\n",
    "    for chunking_type in chunking_types:\n",
    "\n",
    "        if chunking_type == \"fixed_number\":\n",
    "            for chunk_size in chunk_sizes:\n",
    "                for chunk_overlap in chunk_overlaps:\n",
    "                    run_tests(chunking_type = chunking_type, \n",
    "                            semantic_chunking_type = None, \n",
    "                            model_name = model_name,\n",
    "                            chunk_size = chunk_size,\n",
    "                            chunk_overlap = chunk_overlap,\n",
    "                            eval_dataset = eval_dataset,\n",
    "                            generator_name = GENERATOR_MODEL_NAME)\n",
    "                    \n",
    "        elif chunking_type == \"semantic\":\n",
    "            for semantic_chunking_type in semantic_chunking_types:\n",
    "                run_tests(chunking_type = chunking_type, \n",
    "                        semantic_chunking_type = semantic_chunking_type, \n",
    "                        model_name = model_name,\n",
    "                        chunk_size = None,\n",
    "                        chunk_overlap = None,\n",
    "                        eval_dataset = eval_dataset,\n",
    "                        generator_name = GENERATOR_MODEL_NAME)\n",
    "                \n",
    "        else:\n",
    "            run_tests(chunking_type = chunking_type, \n",
    "                    semantic_chunking_type = None, \n",
    "                    model_name = model_name,\n",
    "                    chunk_size = None,\n",
    "                    chunk_overlap = None,\n",
    "                    eval_dataset = eval_dataset,\n",
    "                    generator_name = GENERATOR_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from ragas.metrics import context_precision, answer_relevancy, faithfulness, context_recall, answer_correctness\n",
    "from ragas.run_config import RunConfig\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o\"\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model=\"TextEmbedding3LargeDeployment\",\n",
    "    api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "chunking_type = \"page_chunking\"\n",
    "chunk_size = 384\n",
    "chunk_overlap = 0\n",
    "model_name = \"multilingual\"\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "                \n",
    "if chunking_type == \"page_chunking\":\n",
    "    chunking = chunking_type\n",
    "elif chunking_type == \"semantic\":\n",
    "    chunking = f\"{chunking_type}_{semantic_chunking_type}\"\n",
    "else:\n",
    "    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "settings_name = f\"rag_chunk:{chunking}_embeddings:{model_name}_reader-model:{GENERATOR_MODEL_NAME}\"\n",
    "folder = f\"output/{GENERATOR_MODEL_NAME}/{settings_name}\"\n",
    "\n",
    "with open(f\"{folder}/dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "d = {\n",
    "    \"question\": [entry[\"question\"] for entry in dataset],\n",
    "    \"contexts\": [entry[\"retrieved_docs\"] for entry in dataset],\n",
    "    \"answer\": [entry[\"generated_answer\"] for entry in dataset],\n",
    "    \"ground_truth\": [entry[\"true_answer\"] for entry in dataset],\n",
    "}\n",
    "\n",
    "eval_dataset = Dataset.from_dict(d)\n",
    "\n",
    "if not os.path.exists(f\"{folder}/results.json\"):\n",
    "    run_config = RunConfig(timeout = 6000, max_retries = 20, max_wait = 50, log_tenacity = False)\n",
    "    print(settings_name)\n",
    "    results = evaluate(dataset = eval_dataset, \n",
    "                    metrics = [context_precision, faithfulness, answer_relevancy, context_recall, answer_correctness], \n",
    "                    llm = llm, \n",
    "                    embeddings = embeddings,\n",
    "                    run_config = run_config)\n",
    "    \n",
    "    results.to_pandas().to_json(f\"{folder}/results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS results processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from statistics import mean, variance\n",
    "\n",
    "\n",
    "path = f\"{folder}/results.json\"\n",
    "dataset = Dataset.from_json(path)\n",
    "metrics = {}\n",
    "\n",
    "for metric in [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\", \"answer_correctness\"]:\n",
    "    for d in dataset[metric]:\n",
    "        metrics[metric] = {\"mean\": mean([ d[entry] if d[entry] != None else 0.0 for entry in d ]),\n",
    "            \"variance\": variance([ d[entry] if d[entry] != None else 0.0 for entry in d ]),                  \n",
    "        }\n",
    "\n",
    "    print(f\"{metric}:\\n{metrics[metric][\"mean\"]:.3f}, {metrics[metric][\"variance\"]:.3f}\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "for d in dataset[\"response\"]:\n",
    "    for k in d:\n",
    "        print(f\"{k}: {d[k]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect post-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "path = f\"{folder}/results.json\"\n",
    "dataset = Dataset.from_json(path)\n",
    "\n",
    "print(path)\n",
    "\n",
    "key = \"2\"\n",
    "# print(f\"{dataset[\"user_input\"][0][key]}\")\n",
    "# print(f\"{dataset[\"reference\"][0][key]}\")\n",
    "print(f\"{dataset[\"response\"][0][key]}\")\n",
    "print(f\"{dataset[\"context_precision\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"faithfulness\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"answer_relevancy\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"context_recall\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"answer_correctness\"][0][key]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
