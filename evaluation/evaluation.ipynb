{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAG generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o-mini\",\n",
    "    max_tokens = 256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum, keep the answer concise and DO NOT mention the context and from which documents you take information.\n",
    "    Question: {question}\\n \n",
    "    Context: {context}\\n \n",
    "    Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard-filtered RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HARD_FILTERED_RAG_METADATA_EXTRACTOR = \"\"\"\n",
    "    You are tasked to extract useful info from a query provided. \n",
    "    These info are: year, country, the country's continent and target (clinic or lab). \n",
    "    If the country is not specified, country and continent will be \"Europe\"; if the target is not specified it will be \"all\".\n",
    "    Query: {query}\\n\n",
    "    Country:\n",
    "    Continent:\n",
    "    Year:\n",
    "    Target:\n",
    "\"\"\"\n",
    "\n",
    "def context_filter(llm_metadata_output: str):\n",
    "\n",
    "    metadata = {}\n",
    "    fields = llm_metadata_output.replace(\" \", \"\").split(\"\\n\")\n",
    "    metadata[\"country\"] = fields[0].split(\":\")[-1]\n",
    "    metadata[\"continent\"] = fields[1].split(\":\")[-1]\n",
    "    metadata[\"year\"] = fields[2].split(\":\")[-1]\n",
    "    metadata[\"target\"] = fields[3].split(\":\")[-1]\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.docstore.document import Document\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: InMemoryVectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    "    rag_type: str = \"RAG_simple\"\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    \n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    \n",
    "    match rag_type:\n",
    "        case \"RAG_simple\":\n",
    "            relevant_docs = knowledge_index.similarity_search(query = question, k = num_retrieved_docs)\n",
    "\n",
    "        case \"RAG_filter_hard\":\n",
    "            llm_metadata_output = llm.invoke(HARD_FILTERED_RAG_METADATA_EXTRACTOR.format(query = question)).content\n",
    "            metadata = context_filter(llm_metadata_output)\n",
    "            year = metadata[\"year\"]\n",
    "            country = metadata[\"country\"]\n",
    "            continent = metadata[\"continent\"]\n",
    "            target = metadata[\"target\"] if metadata[\"target\"] != \"all\" else \"\"\n",
    "            relevant_docs = knowledge_index.similarity_search(query = question, \n",
    "                                                              k = num_retrieved_docs, \n",
    "                                                              filter = lambda doc: \n",
    "                                                              f\"{year}\" in doc.metadata[\"keywords\"] and\n",
    "                                                              (f\"{country}\" in doc.metadata[\"keywords\"] or f\"{continent}\" in doc.metadata[\"keywords\"]) and \n",
    "                                                              f\"{target}\" in doc.metadata[\"keywords\"] if target == \"lab\" or target == \"clinic\" else True)\n",
    "            \n",
    "        case _: \n",
    "            return\n",
    "           \n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    final_prompt = SIMPLE_RAG_PROMPT_TEMPLATE.format(question = question, context = context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm.invoke(final_prompt).content\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG tests runner (simple RAG, RAG with hard filters, RAG with llm filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from datasets import Dataset\n",
    "from typing import Optional\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "    k = 4, \n",
    "    k_final = 4,\n",
    "    rag_type: str = \"RAG_simple\"\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(question, llm, \n",
    "                                                knowledge_index, \n",
    "                                                reranker = reranker,\n",
    "                                                num_retrieved_docs = k,\n",
    "                                                num_docs_final = k_final, \n",
    "                                                rag_type = rag_type)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, indent = 4, ensure_ascii = False)\n",
    "\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "all_embeddings = {     \n",
    "    # \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    # \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    # \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "        model = \"TextEmbedding3LargeDeployment\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        chunk_size = 384\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate answers with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def run_tests(chunking_type, \n",
    "              semantic_chunking_type, \n",
    "              model_name, chunk_size, \n",
    "              chunk_overlap, \n",
    "              eval_dataset, \n",
    "              generator_name,\n",
    "              rag_type):\n",
    "\n",
    "    if chunking_type == \"page_chunking\":\n",
    "        chunking = chunking_type\n",
    "    elif chunking_type == \"semantic\":\n",
    "        chunking = f\"{chunking_type}_{semantic_chunking_type}\"\n",
    "    else:\n",
    "        chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "    settings_name = f\"chunk:{chunking}_embeddings:{model_name}_reader-model:{generator_name}\"\n",
    "    output_file_name = f\"./output/{generator_name}/Text+Images/{rag_type}/rag_{settings_name}/dataset_countries&years_brands.json\"\n",
    "\n",
    "    if os.path.exists(output_file_name):\n",
    "        return\n",
    "    if not os.path.exists(f\"./output/{generator_name}/Text+Images/{rag_type}/rag_{settings_name}\"):\n",
    "        os.mkdir(f\"./output/{generator_name}/Text+Images/{rag_type}/rag_{settings_name}\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_file_name, \"r\"):\n",
    "            pass\n",
    "    except:\n",
    "        print(\"Running RAG...\")\n",
    "        print(f\"Configuration: model: {model_name}, chunking: {chunking}\")\n",
    "        reranker = None\n",
    "        vector_store_path = f\"../indexing/models/Text+Images/{model_name}/{chunking}/{chunking}_{model_name}\"\n",
    "        embeddings = all_embeddings[model_name]\n",
    "        vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "        run_rag_tests(\n",
    "            eval_dataset = eval_dataset,\n",
    "            llm = llm,\n",
    "            knowledge_index = vector_store,\n",
    "            output_file = output_file_name,\n",
    "            reranker = reranker,\n",
    "            verbose = False,\n",
    "            test_settings = settings_name,\n",
    "            k = 4,\n",
    "            k_final = 4,\n",
    "            rag_type = rag_type\n",
    "        )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "RAG_type = \"RAG_filter_hard\"\n",
    "chunking_types = [\"page_chunking\", \"fixed_number\"]\n",
    "chunk_sizes = [384, 256]\n",
    "chunk_overlaps = [0, 20, 50, 100]\n",
    "semantic_chunking_types = [\"percentile\", \"interquartile\", \"gradient\"]\n",
    "model_names = all_embeddings.keys()\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "with open(\"dataset/all_QA_countries&years_brands.json\", \"r\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "for model_name in model_names:\n",
    "    for chunking_type in chunking_types:\n",
    "\n",
    "        if chunking_type == \"fixed_number\":\n",
    "            for chunk_size in chunk_sizes:\n",
    "                for chunk_overlap in chunk_overlaps:\n",
    "                    run_tests(chunking_type = chunking_type, \n",
    "                            semantic_chunking_type = None, \n",
    "                            model_name = model_name,\n",
    "                            chunk_size = chunk_size,\n",
    "                            chunk_overlap = chunk_overlap,\n",
    "                            eval_dataset = eval_dataset,\n",
    "                            generator_name = GENERATOR_MODEL_NAME,\n",
    "                            rag_type = RAG_type)\n",
    "                    \n",
    "        elif chunking_type == \"semantic\":\n",
    "            for semantic_chunking_type in semantic_chunking_types:\n",
    "                run_tests(chunking_type = chunking_type, \n",
    "                        semantic_chunking_type = semantic_chunking_type, \n",
    "                        model_name = model_name,\n",
    "                        chunk_size = None,\n",
    "                        chunk_overlap = None,\n",
    "                        eval_dataset = eval_dataset,\n",
    "                        generator_name = GENERATOR_MODEL_NAME,\n",
    "                        rag_type = RAG_type)\n",
    "                \n",
    "        else:\n",
    "            run_tests(chunking_type = chunking_type, \n",
    "                    semantic_chunking_type = None, \n",
    "                    model_name = model_name,\n",
    "                    chunk_size = None,\n",
    "                    chunk_overlap = None,\n",
    "                    eval_dataset = eval_dataset,\n",
    "                    generator_name = GENERATOR_MODEL_NAME,\n",
    "                    rag_type = RAG_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:page_chunking_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee97f6e6f6148a6b7d3d8abc489533f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[13]: RateLimitError(Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}})\n",
      "Exception raised in Job[11]: RateLimitError(Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}})\n",
      "Exception raised in Job[31]: RateLimitError(Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}})\n",
      "Exception raised in Job[33]: RateLimitError(Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-08-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:256_0_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc932b0166d4fa8b4b8c6463ac2ce2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:256_20_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce75e479dab4e15857a634c0dcff029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:256_50_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb5e121496b42ce81fb7651809d1c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:256_100_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993283af04c94949a0ee16dc236f6be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:384_0_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63355ed06a54d129bf17615eb6b77a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:384_20_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b746eefd7484ba2b13d833acbc62f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:384_50_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f049beaf1d63446ab1f82d38509f6a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:384_100_embeddings:multilingual_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdcb862176748f3b8a2e7ba3160b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:page_chunking_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90195150b7c84860891b30493ef57425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:256_0_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cc2e3429054b4a8a032948e88babf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:256_20_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0dbae881f0405687958fe9215e28c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:256_50_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e745a66869574b96819a5a23da895ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:256_100_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cbd303a62e4839b3355681d7418d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:384_0_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5511b6fe9644a74b90e90da6fb05443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:384_20_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1980bf7141744ba833874500c3828fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:384_50_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d980f13562a4330a64be7e32c3f9df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_chunk:384_100_embeddings:text_embedding_3_large_reader-model:GPT_4o_mini\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a6f0f5996542e996c987eb47a8a975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from ragas.metrics import context_precision, answer_relevancy, faithfulness, context_recall, answer_correctness\n",
    "from ragas.run_config import RunConfig\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o\"\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model=\"TextEmbedding3LargeDeployment\",\n",
    "    api_version=\"2023-05-15\"\n",
    ")\n",
    "\n",
    "chunking_type = \"page_chunking\"\n",
    "chunk_size = 384\n",
    "chunk_overlap = 0\n",
    "model_name = \"multilingual\"\n",
    "GENERATOR_MODEL_NAME = \"GPT_4o_mini\"\n",
    "\n",
    "for model_name in [\"multilingual\", \"text_embedding_3_large\"]:\n",
    "    for chunking_type in [\"page_chunking\", \"fixed_number\"]:\n",
    "        for chunk_size in [256, 384]:\n",
    "            for chunk_overlap in [0, 20, 50, 100]:\n",
    "\n",
    "                if chunking_type == \"page_chunking\":\n",
    "                    chunking = chunking_type\n",
    "                elif chunking_type == \"semantic\":\n",
    "                    chunking = f\"{chunking_type}_{semantic_chunking_type}\"\n",
    "                else:\n",
    "                    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "                settings_name = f\"rag_chunk:{chunking}_embeddings:{model_name}_reader-model:{GENERATOR_MODEL_NAME}\"\n",
    "                folder = f\"output/{GENERATOR_MODEL_NAME}/Text+Images/RAG_simple/{settings_name}\"\n",
    "\n",
    "                with open(f\"{folder}/dataset_countries&years_brands.json\", \"r\") as f:\n",
    "                    dataset = json.load(f)\n",
    "\n",
    "                # Take a subset\n",
    "                dataset = dataset[:15]\n",
    "\n",
    "                d = {\n",
    "                    \"question\": [entry[\"question\"] for entry in dataset],\n",
    "                    \"contexts\": [entry[\"retrieved_docs\"] for entry in dataset],\n",
    "                    \"answer\": [entry[\"generated_answer\"] for entry in dataset],\n",
    "                    \"ground_truth\": [entry[\"true_answer\"] for entry in dataset],\n",
    "                }\n",
    "\n",
    "                eval_dataset = Dataset.from_dict(d)\n",
    "\n",
    "                if not os.path.exists(f\"{folder}/results_countries&years_brands.json\"):\n",
    "                    run_config = RunConfig(timeout = 6000, max_retries = 20, max_wait = 50, log_tenacity = False)\n",
    "                    print(settings_name)\n",
    "                    results = evaluate(dataset = eval_dataset, \n",
    "                                    metrics = [context_precision, faithfulness, answer_relevancy, context_recall, answer_correctness], \n",
    "                                    llm = llm, \n",
    "                                    embeddings = embeddings,\n",
    "                                    run_config = run_config)\n",
    "                    \n",
    "                    results.to_pandas().to_json(f\"{folder}/results_countries&years_brands.json\", indent = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAGAS results processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from statistics import mean, variance\n",
    "\n",
    "\n",
    "path = f\"{folder}/results.json\"\n",
    "dataset = Dataset.from_json(path)\n",
    "metrics = {}\n",
    "\n",
    "for metric in [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\", \"answer_correctness\"]:\n",
    "    for d in dataset[metric]:\n",
    "        metrics[metric] = {\"mean\": mean([ d[entry] if d[entry] != None else 0.0 for entry in d ]),\n",
    "            \"variance\": variance([ d[entry] if d[entry] != None else 0.0 for entry in d ]),                  \n",
    "        }\n",
    "\n",
    "    print(f\"{metric}:\\n{metrics[metric][\"mean\"]:.3f}, {metrics[metric][\"variance\"]:.3f}\\n\")\n",
    "\n",
    "print(\"\\n\")\n",
    "for d in dataset[\"response\"]:\n",
    "    for k in d:\n",
    "        print(f\"{k}: {d[k]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect post-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "path = f\"{folder}/results.json\"\n",
    "dataset = Dataset.from_json(path)\n",
    "\n",
    "print(path)\n",
    "\n",
    "key = \"2\"\n",
    "# print(f\"{dataset[\"user_input\"][0][key]}\")\n",
    "# print(f\"{dataset[\"reference\"][0][key]}\")\n",
    "print(f\"{dataset[\"response\"][0][key]}\")\n",
    "print(f\"{dataset[\"context_precision\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"faithfulness\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"answer_relevancy\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"context_recall\"][0][key]:.3f}\")\n",
    "print(f\"{dataset[\"answer_correctness\"][0][key]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
