{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case document content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes non ASCII characters (TO DO: apply only if document language = english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes bulleted and numbered lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•\\-*✔●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes multiple consecutive escape characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed chunks in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_chunks(pages: list, path: str):\n",
    "\n",
    "    # path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    # full_path = \"\"\n",
    "\n",
    "    # match type:\n",
    "    #     case \"r\":\n",
    "    #         full_path = path + \"/raw/chunk_\"\n",
    "    #     case \"c\":\n",
    "    #         full_path = path + \"/cleaned/chunk_\"\n",
    "    #     case _:\n",
    "    #         return\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for chunk in range(len(pages)):\n",
    "        full_path = path + \"/\" + \"chunk_\" + str(chunk + 1)\n",
    "        with open(full_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed chunks (raw, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.load import load\n",
    "\n",
    "def load_chunks(path: str):\n",
    "\n",
    "    # path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    # full_path = \"\"\n",
    "    pages = []\n",
    "    # match type:\n",
    "    #     case \"r\": \n",
    "    #         full_path = path + \"/raw\"\n",
    "    #     case \"c\":\n",
    "    #         full_path = path + \"/cleaned\"\n",
    "    #     case _:\n",
    "    #         return None\n",
    "\n",
    "    try:   \n",
    "        for fname in os.listdir(path):\n",
    "            f = os.path.join(path, fname)\n",
    "            with open(f, \"r\") as file:\n",
    "                page = load(json.load(file))\n",
    "                pages.append(page)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all file from source folder to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# processed_files = [\n",
    "#     'BEQ_2301_OVERALL_multi.pdf', \n",
    "#     'CADCAM_BRA_22_Eng.pdf', \n",
    "#     'IOS_Report_FR-IT-ES_rev17.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_LAB_Executive_Summary_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_LAB_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_17_CLI_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_23_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_ITA_19_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_LAB_.pdf'\n",
    "# ]\n",
    "processed_files = []\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        if fname not in processed_files:\n",
    "            files.append(complete_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "chunking_type = \"fixed_number\"\n",
    "chunk_size = 384\n",
    "chunk_overlap = 100\n",
    "\n",
    "all_chunkings = {\n",
    "    \"page_chunking\": None,\n",
    "    \"fixed_number\": TokenTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents and tables within into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "path = \"chunkings/No OCR/\"\n",
    "\n",
    "match chunking_type:\n",
    "    case \"page_chunking\":\n",
    "        path = path + chunking_type + \"/raw\" \n",
    "\n",
    "    case \"fixed_number\":\n",
    "        path = path + str(chunk_size) + \"_\" + str(chunk_overlap)\n",
    "\n",
    "    case _:\n",
    "        pass\n",
    "    \n",
    "pages = load_chunks(path)\n",
    "\n",
    "text_splitter = all_chunkings[chunking_type]\n",
    "\n",
    "# Index new documents  \n",
    "new_pages = []\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file, extract_images = False)\n",
    "\n",
    "    if chunking_type == \"page_chunking\":\n",
    "        async for page in loader.alazy_load():\n",
    "            new_pages.append(page)\n",
    "            \n",
    "    else:\n",
    "        for page in loader.load_and_split(text_splitter = text_splitter):\n",
    "            new_pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282\n"
     ]
    }
   ],
   "source": [
    "print(len(new_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661.2316692667707\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "print(mean([len(p.page_content) for p in new_pages]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process content (text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "new_cleaned_pages = copy.deepcopy(new_pages)\n",
    "\n",
    "remove_non_ASCII(new_cleaned_pages)\n",
    "decapitalize_content(new_cleaned_pages)\n",
    "remove_bullets(new_cleaned_pages)\n",
    "remove_escape(new_cleaned_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge new pages with existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages += new_pages\n",
    "# cleaned_pages += new_cleaned_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"chunkings/No OCR/\" + str(chunk_size) + \"_\" + str(chunk_overlap)\n",
    "save_chunks(pages, path)\n",
    "\n",
    "# Update processed files list\n",
    "processed_files += [file.replace(folder + \"/\", \"\") for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "chunking = chunking_type + \"_\" + str(chunk_overlap)\n",
    "\n",
    "all_embeddings = {\n",
    "    # \"llama1b\": \n",
    "    # {\n",
    "    #     \"model\": OllamaEmbeddings(model = \"llama3.2:1b\"),\n",
    "    #     \"raw-path\": \"models/No OCR/llama3.2:1b/raw/llama3.2:1b_raw\",\n",
    "    #     \"cleaned-path\": \"models/No OCR/llama3.2:1b/cleaned/llama3.2:1b_cleaned\"\n",
    "    # }, \n",
    "    # \"llama3b\": \n",
    "    # {\n",
    "    #     \"model\": OllamaEmbeddings(model = \"llama3.2:3b\"),\n",
    "    #     \"raw-path\": \"models/No OCR/llama3.2:3b/raw/llama3.2:3b_raw\",\n",
    "    #     \"cleaned-path\": \"models/No OCR/llama3.2:3b/cleaned/llama3.2:3b_cleaned\"\n",
    "    # },\n",
    "    # \"gemma2b\": \n",
    "    # {\n",
    "    #     \"model\": OllamaEmbeddings(model = \"gemma2:2b\"),\n",
    "    #     \"raw-path\": \"models/No OCR/gemma2b/raw/gemma2b_raw\",\n",
    "    #     \"cleaned-path\": \"models/No OCR/gemma2b/cleaned/gemma2b_cleaned\"\n",
    "    # },\n",
    "    \"mpnetbase\": \n",
    "    {\n",
    "        \"model\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "        \"raw-path\": \"models/No OCR/mpnet_base_v2/\" + chunking + \"/mpnet_base_v2\",\n",
    "        \"cleaned-path\": \"models/No OCR/mpnet_base_v2/\" + chunking + \"/mpnet_base_v2_cleaned\"\n",
    "    },\n",
    "    \"minilm-l6\": \n",
    "    {\n",
    "        \"model\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-l6-v2\"),\n",
    "        \"raw-path\": \"models/No OCR/minilm-l6/\" + chunking + \"/minilm_l6_raw\",\n",
    "        \"cleaned-path\": \"models/No OCR/minilm-l6/\" + chunking + \"/minilm_l6_cleaned\"\n",
    "    },\n",
    "    \"minilm-l12\":\n",
    "    {\n",
    "        \"model\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "        \"raw-path\": \"models/No OCR/minilm-l12/\" + chunking + \"/minilm_l12_raw\",\n",
    "        \"cleaned-path\": \"models/No OCR/minilm-l12/\" + chunking + \"/minilm_l12_cleaned\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import os\n",
    "\n",
    "model_name = \"mpnetbase\"\n",
    "embeddings = all_embeddings[model_name][\"model\"]\n",
    "vector_store_path = all_embeddings[model_name][\"cleaned-path\"]\n",
    "vector_store = None\n",
    "\n",
    "if os.path.exists(vector_store_path):\n",
    "    vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "\n",
    "else: \n",
    "    vector_store = InMemoryVectorStore.from_documents(documents = pages, embedding = embeddings)\n",
    "    vector_store.dump(vector_store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [\"Intra oral scanner\", \"3D printer\"]\n",
    "producers = [\"Dentsply Sirona\", \"Kavo\", \"3M\", \"GC\", \"Ivoclar\", \"Straumann\", \"Kulzer\", \"Voco\"]\n",
    "intervals = [1, 2, 3, 4]\n",
    "countries = [\"Italy\", \"Germany\", \"Spain\", \"UK\", \"United Kingdom\", \"Brazil\"]\n",
    "\n",
    "all_queries = [\n",
    "    \"Trend of inflation in the dental sector between 2021, 2022, and the first half of 2023\",\n",
    "    \"Dental product brands that offer the best value for money according to dentists\",\n",
    "    \"Which are the most relevant dental brands?\",\n",
    "    \"Which are the most recommended products?\",\n",
    "    \"What are the preferred purchasing channels in different countries?\",\n",
    "    f\"Evolution of {products[1]} adoption\",\n",
    "    f\"Which is the country where {products[0]} is most successful?\",\n",
    "    f\"Evolution of {producers[0]}'s loyalitization capability\",\n",
    "    f\"Evolution of {products[0]}'s market in the last {intervals[2]} years\",\n",
    "    f\"Difference in {products[1]} adoption between {countries[0]} and {countries[4]}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = all_queries[5]\n",
    "docs = vector_store.similarity_search_with_score(query, k = 4)\n",
    "\n",
    "print(\"Query: \" + query)\n",
    "for doc in docs:\n",
    "    print(doc[1])\n",
    "    print(\"Sorgente: \" + doc[0].metadata[\"source\"] + \", pagina: \" + str(doc[0].metadata[\"page\"]))\n",
    "    print(doc[0].page_content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on a list of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, variance\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for query in all_queries:\n",
    "    docs_relevances = vector_store.similarity_search_with_score(query, k = 10)\n",
    "    all_scores.append(docs_relevances[0][1])\n",
    "\n",
    "avg = mean(all_scores)\n",
    "var = variance(all_scores, avg)\n",
    "\n",
    "print(f\"{avg:.3f}, {var:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute number of different retrieved chunks between two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "model_name_1 = \"minilm-l6\"\n",
    "model_name_2 = \"minilm-l12\"\n",
    "embeddings_1 = all_embeddings[model_name_1][\"model\"]\n",
    "embeddings_2 = all_embeddings[model_name_2][\"model\"]\n",
    "\n",
    "vector_store_path_1 = all_embeddings[model_name_1][\"cleaned-path\"]\n",
    "vector_store_path_2 = all_embeddings[model_name_2][\"cleaned-path\"]\n",
    "\n",
    "vector_store_1 = InMemoryVectorStore.load(path = vector_store_path_1, embedding = embeddings_1)\n",
    "vector_store_2 = InMemoryVectorStore.load(path = vector_store_path_2, embedding = embeddings_2)\n",
    "\n",
    "diff = 0\n",
    "\n",
    "for query in all_queries:\n",
    "\n",
    "    chunks_1 = vector_store_1.similarity_search(query, k = 4)\n",
    "    chunks_2 = vector_store_2.similarity_search(query, k = 4)\n",
    "\n",
    "    chunks_mapped_1 = list(map(lambda c: (c.metadata[\"source\"], c.metadata[\"page\"]), chunks_1))\n",
    "    chunks_mapped_2 = list(map(lambda c: (c.metadata[\"source\"], c.metadata[\"page\"]), chunks_2))\n",
    "\n",
    "    diff += sum([1 for tuple in chunks_mapped_1 if tuple not in chunks_mapped_2])\n",
    "\n",
    "print(diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
