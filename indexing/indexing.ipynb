{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case document content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes non ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        if \"non-en\" not in p.metadata[\"keywords\"]:\n",
    "            p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes bulleted and numbered lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•\\-*✔●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes multiple consecutive escape characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed chunks in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_chunks(pages: list, path: str):\n",
    "\n",
    "    # path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    # full_path = \"\"\n",
    "\n",
    "    # match type:\n",
    "    #     case \"r\":\n",
    "    #         full_path = path + \"/raw/chunk_\"\n",
    "    #     case \"c\":\n",
    "    #         full_path = path + \"/cleaned/chunk_\"\n",
    "    #     case _:\n",
    "    #         return\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for chunk in range(len(pages)):\n",
    "        full_path = path + \"/\" + \"chunk_\" + str(chunk + 1)\n",
    "        with open(full_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed chunks (raw, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.load import load\n",
    "\n",
    "def load_chunks(path: str):\n",
    "\n",
    "    # path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    # full_path = \"\"\n",
    "    pages = []\n",
    "    # match type:\n",
    "    #     case \"r\": \n",
    "    #         full_path = path + \"/raw\"\n",
    "    #     case \"c\":\n",
    "    #         full_path = path + \"/cleaned\"\n",
    "    #     case _:\n",
    "    #         return None\n",
    "\n",
    "    try:   \n",
    "        for fname in os.listdir(path):\n",
    "            f = os.path.join(path, fname)\n",
    "            with open(f, \"r\") as file:\n",
    "                page = load(json.load(file))\n",
    "                pages.append(page)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter metadata (this step is document-specific and may change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pikepdf\n",
    "\n",
    "# This document is written in Spanish, so we decide to mark it into metadata\n",
    "pdf = pikepdf.Pdf.open(\"../sources/OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf\",\n",
    "                  allow_overwriting_input = True)\n",
    "\n",
    "edited = False  \n",
    "with pdf.open_metadata() as meta:\n",
    "    if meta[\"keywords\"] == \"\":\n",
    "        meta[\"keywords\"] = \"non-en\"\n",
    "        edited = True\n",
    "        \n",
    "if edited:  \n",
    "    pdf.save(\"../sources/OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all file from source folder to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# processed_files = [\n",
    "#     'BEQ_2301_OVERALL_multi.pdf', \n",
    "#     'CADCAM_BRA_22_Eng.pdf', \n",
    "#     'IOS_Report_FR-IT-ES_rev17.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_LAB_Executive_Summary_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_LAB_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_17_CLI_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_23_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_ITA_19_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_LAB_.pdf'\n",
    "# ]\n",
    "processed_files = []\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        if fname not in processed_files:\n",
    "            files.append(complete_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "chunking_type = \"semantic\"\n",
    "\n",
    "chunk_size = 256\n",
    "chunk_overlap = 100\n",
    "\n",
    "semantic_chunking_type = \"interquartile\"\n",
    "semantic_chunking_model = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name = semantic_chunking_model)\n",
    "\n",
    "all_chunkings = {\n",
    "    \"page_chunking\": None,\n",
    "    \"fixed_number\": TokenTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap),\n",
    "    \"semantic\": SemanticChunker(embeddings = embeddings, \n",
    "                                breakpoint_threshold_type = semantic_chunking_type)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents and tables within into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "path = \"chunkings/No OCR/\"\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "    path = path + chunking_type + \"/cleaned\" \n",
    "elif chunking_type == \"semantic\":\n",
    "    path = path + chunking_type + \"_\" + semantic_chunking_type + \"_\" + \"minilm_l12\"\n",
    "else:\n",
    "    path = path + str(chunk_size) + \"_\" + str(chunk_overlap)\n",
    "    \n",
    "pages = load_chunks(path)\n",
    "\n",
    "text_splitter = all_chunkings[chunking_type]\n",
    "\n",
    "# Index new documents  \n",
    "new_pages = []\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file, extract_images = False)\n",
    "\n",
    "    if chunking_type == \"page_chunking\":\n",
    "        async for page in loader.alazy_load():\n",
    "            new_pages.append(page)\n",
    "            \n",
    "    else:\n",
    "        for page in loader.load_and_split(text_splitter = text_splitter):\n",
    "            new_pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chunkings/No OCR/semantic_interquartile_minilm_l12'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pages))\n",
    "print(len(new_pages))\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process content (text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_non_ASCII(new_pages)\n",
    "decapitalize_content(new_pages)\n",
    "remove_bullets(new_pages)\n",
    "remove_escape(new_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge new pages with existing ones and serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages += new_pages\n",
    "\n",
    "save_chunks(pages, path)\n",
    "\n",
    "# Update processed files list\n",
    "processed_files += [file.replace(folder + \"/\", \"\") for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "all_embeddings = {\n",
    "    \"llama3.2:1b\": OllamaEmbeddings(model = \"llama3.2:1b\"),\n",
    "     \n",
    "    \"llama3.2:3b\": OllamaEmbeddings(model = \"llama3.2:3b\"),\n",
    "     \n",
    "    \"gemma2b\": OllamaEmbeddings(model = \"llama3.2:1b\"),\n",
    "     \n",
    "    \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import os\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "    chunking = chunking_type\n",
    "elif chunking_type == \"semantic\":\n",
    "    chunking = chunking_type + \"_\" + semantic_chunking_type + \"_minilm_l12\"\n",
    "else:\n",
    "    chunking = str(chunk_size) + \"_\" + str(chunk_overlap)\n",
    "\n",
    "model_name = \"minilm_l12\"\n",
    "embeddings = all_embeddings[model_name]\n",
    "vector_store_path = \"models/No OCR/\" + model_name + \"/\" + chunking + \"/\" + model_name\n",
    "vector_store = None\n",
    "\n",
    "if os.path.exists(vector_store_path):\n",
    "    vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "\n",
    "else: \n",
    "    vector_store = InMemoryVectorStore.from_documents(documents = pages, embedding = embeddings)\n",
    "    vector_store.dump(vector_store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [\"Intra oral scanner\", \"3D printer\"]\n",
    "producers = [\"Dentsply Sirona\", \"Kavo\", \"3M\", \"GC\", \"Ivoclar\", \"Straumann\", \"Kulzer\", \"Voco\"]\n",
    "intervals = [1, 2, 3, 4]\n",
    "countries = [\"Italy\", \"Germany\", \"Spain\", \"UK\", \"United Kingdom\", \"Brazil\"]\n",
    "\n",
    "all_queries = [\n",
    "    \"Trend of inflation in the dental sector between 2021, 2022, and the first half of 2023\",\n",
    "    \"Dental product brands that offer the best value for money according to dentists\",\n",
    "    \"Which are the most relevant dental brands?\",\n",
    "    \"Which are the most recommended products?\",\n",
    "    \"What are the preferred purchasing channels in different countries?\",\n",
    "    f\"Evolution of {products[1]} adoption\",\n",
    "    f\"Which is the country where {products[0]} is most successful?\",\n",
    "    f\"Evolution of {producers[0]}'s loyalitization capability\",\n",
    "    f\"Evolution of {products[0]}'s market in the last {intervals[2]} years\",\n",
    "    f\"Difference in {products[1]} adoption between {countries[0]} and {countries[4]}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Difference in 3D printer adoption between Italy and United Kingdom\n",
      "0.7075279448144062\n",
      "Sorgente: ../sources/OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf, pagina: 28\n",
      "it is worth to underline the higher-than-average penetration in germany and uk, while a lower-than-average intention to buy can be noted in italy and france. sales of 3d printers have shown a steep increase in recent years (75% of purchases have been made since 2019), mainly due to improvements in precision, production times and function of use. clinic year of purchase average: 2019 base: 1.061 cases until a few years ago 3d printers were used exclusively for printing individual impression trays and surgical guides; now they are also quoted to be used for printing temporaries and the production of the surgical guides has increased considerably; in addition, some models are capable of processing zirconia or lithium disilicate, thus potentially competing with milling units in the future. germany france italy spain uk yes, and we use it often 13% 2% 4% 5% 16% yes, but we rarely use it 5% 3% 6% 4% 6% no, but we plan to buy it 27% 19% 0% 26% 20% no, and we don't plan to buy it 13% 2% 2% 2% 1% no, and we don't know if we will buy it 43% 74% 88% 62% 57% cases 256 224 200 201 180\n",
      "\n",
      "0.6274863838786262\n",
      "Sorgente: ../sources/OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf, pagina: 29\n",
      "3d printers brands what are the brand and the year of purchase of your 3d printer? 30 base: 1.061 cases as regards the indications for use of the 3d printers, past surveys also indicate a slight shift in the procedures: while a few years ago the most suitable indication was related to temporary c&b, today the most quoted production procedure is surgical guides, followed by aligners. among practices equipped with a 3d printer, formlabs is the leading brand in penetration, followed by anycubic. as regards the use of the machines, it seems that in germany, and partially in the uk, there is a more transversal use across all applications. in italy and spain, the technology is mainly used for the fabrication of surgical guides and temporary elements. it is worth mentioning that in these two countries in 2018, the 3d printer penetration in dental practices was around 1%. what are the main processes you carry out with the 3d printer you have in your clinic? clinic do you have a 3d printer in your dental practice? germany france italy spain uk penetration 17% 5% 10% 10% 23% 3d systems formlabs anycubic formlabs formlabs envisiontec elegoo formlabs anycubic envisiontec formlabs 3d systems elegoo elegoo kuzer cases 256 224 200 201 180 installed base top 3 ranking\n",
      "\n",
      "0.612648941302828\n",
      "Sorgente: ../sources/BEQ_2301_OVERALL_multi.pdf, pagina: 97\n",
      "3d printing penetration do you have a 3d printer in your dental practice? 98 france germany italy spain uk yes and we use it often 6% 11% 4% 9% 4% yes, but we rarely use it 2% 4% 4% 5% no and we don't know if we will buy it 0% 1% no, but we plan to buy it within a year 15% 17% 26% 31% 4% no and we don't plan to buy it 77% 67% 66% 55% 92% weighted cases 200 224 200 194 200 compared to the last survey, the penetration of 3d printing has not significantly increased. the highest frequency of regular usage of 3d printers is observed in germany. base: 1.018 cases (whole sample)\n",
      "\n",
      "0.5354703878149494\n",
      "Sorgente: ../sources/OMNI_DIGITAL_EU_15_CLI_.pdf, pagina: 32\n",
      "scenario: 3d printing awareness are you aware of the 3d printing technology? are you aware of the availability of 3d printing technology in dentistry as well? 33 base: 1000 cases while the awareness of 3d printing among respondents is quite homogeneous among the countries, the awareness about 3d printing available for dentistry as well is not quite homogeneous as uk shows a relevant below average awareness, while france seems to be the country with the highest awareness. 3d printing in general 3d printing in dentistry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = all_queries[9]\n",
    "docs = vector_store.similarity_search_with_score(query, k = 4)\n",
    "\n",
    "print(\"Query: \" + query)\n",
    "for doc in docs:\n",
    "    print(doc[1])\n",
    "    print(\"Sorgente: \" + doc[0].metadata[\"source\"] + \", pagina: \" + str(doc[0].metadata[\"page\"]))\n",
    "    print(doc[0].page_content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on a list of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.669, 0.007\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, variance\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for query in all_queries:\n",
    "    docs_relevances = vector_store.similarity_search_with_score(query, k = 4)\n",
    "    all_scores.append(docs_relevances[0][1])\n",
    "\n",
    "avg = mean(all_scores)\n",
    "var = variance(all_scores, avg)\n",
    "\n",
    "print(f\"{avg:.3f}, {var:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute number of different retrieved chunks between two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "model_name_1 = \"minilm-l6\"\n",
    "model_name_2 = \"minilm-l12\"\n",
    "embeddings_1 = all_embeddings[model_name_1][\"model\"]\n",
    "embeddings_2 = all_embeddings[model_name_2][\"model\"]\n",
    "\n",
    "vector_store_path_1 = all_embeddings[model_name_1][\"cleaned-path\"]\n",
    "vector_store_path_2 = all_embeddings[model_name_2][\"cleaned-path\"]\n",
    "\n",
    "vector_store_1 = InMemoryVectorStore.load(path = vector_store_path_1, embedding = embeddings_1)\n",
    "vector_store_2 = InMemoryVectorStore.load(path = vector_store_path_2, embedding = embeddings_2)\n",
    "\n",
    "diff = 0\n",
    "\n",
    "for query in all_queries:\n",
    "\n",
    "    chunks_1 = vector_store_1.similarity_search(query, k = 4)\n",
    "    chunks_2 = vector_store_2.similarity_search(query, k = 4)\n",
    "\n",
    "    chunks_mapped_1 = list(map(lambda c: (c.metadata[\"source\"], c.metadata[\"page\"]), chunks_1))\n",
    "    chunks_mapped_2 = list(map(lambda c: (c.metadata[\"source\"], c.metadata[\"page\"]), chunks_2))\n",
    "\n",
    "    diff += sum([1 for tuple in chunks_mapped_1 if tuple not in chunks_mapped_2])\n",
    "\n",
    "print(diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
