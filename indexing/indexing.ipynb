{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case document content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes non ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes bulleted and numbered lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•\\-*✔●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes multiple consecutive escape characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed chunks in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "\n",
    "def save_chunks(pages: list, type: str):\n",
    "\n",
    "    \"\"\"Saves on disk each processed chunk (raw or cleaned), given a flag\n",
    "    * type = \"r\" -> raw chunks\n",
    "    * type = \"c\" -> cleaned chunks \"\"\"\n",
    "\n",
    "    path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    full_path = \"\"\n",
    "\n",
    "    match type:\n",
    "        case \"r\":\n",
    "            full_path = path + \"/raw/chunk_\"\n",
    "        case \"c\":\n",
    "            full_path = path + \"/cleaned/chunk_\"\n",
    "        case _:\n",
    "            return\n",
    "\n",
    "    for chunk in range(len(pages)):\n",
    "        current_path = full_path + str(chunk + 1)\n",
    "        with open(current_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed chunks (raw, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.load import load\n",
    "\n",
    "def load_chunks(type: str = \"r\"):\n",
    "\n",
    "    \"\"\"Retrieves all processed (raw or cleaned) chunks, given a flag\n",
    "    * type = \"r\" -> raw chunks\n",
    "    * type = \"c\" -> cleaned chunks \"\"\"\n",
    "\n",
    "    path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    full_path = \"\"\n",
    "    pages = []\n",
    "\n",
    "    match type:\n",
    "        case \"r\": \n",
    "            full_path = path + \"/raw\"\n",
    "        case \"c\":\n",
    "            full_path = path + \"/cleaned\"\n",
    "        case _:\n",
    "            return None\n",
    "            \n",
    "    for fname in os.listdir(full_path):\n",
    "        f = os.path.join(full_path, fname)\n",
    "        with open(f, \"r\") as file:\n",
    "            page = load(json.load(file))\n",
    "            pages.append(page)\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all file from source folder to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "processed_files = [\n",
    "    'BEQ_2301_OVERALL_multi.pdf', \n",
    "    'CADCAM_BRA_22_Eng.pdf', \n",
    "    'IOS_Report_FR-IT-ES_rev17.pdf', \n",
    "    'OMNI_DIGITAL_EU_15_CLI_.pdf', \n",
    "    'OMNI_DIGITAL_EU_15_CLI_LAB_Executive_Summary_.pdf', \n",
    "    'OMNI_DIGITAL_EU_15_LAB_.pdf', \n",
    "    'OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf', \n",
    "    'OMNI-DIGITAL_ITA_17_CLI_.pdf', \n",
    "    'OMNI-DIGITAL_ITA_23_CLI_.pdf', \n",
    "    'OMNI_DIGITAL_ITA_19_CLI_LAB_INTEGRATED_.pdf', \n",
    "    'OMNI_DIGITAL_SPA_19_CLI_.pdf', \n",
    "    'OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf', \n",
    "    'OMNI_DIGITAL_SPA_19_LAB_.pdf'\n",
    "]\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        if fname not in processed_files:\n",
    "            files.append(complete_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents and tables within into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pages = load_chunks(\"r\")\n",
    "cleaned_pages = load_chunks(\"c\")\n",
    "\n",
    "# Index new documents  \n",
    "new_pages = []\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file, extract_images = False)\n",
    "    async for page in loader.alazy_load():\n",
    "        new_pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process content (text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "new_cleaned_pages = copy.deepcopy(new_pages)\n",
    "\n",
    "remove_non_ASCII(new_cleaned_pages)\n",
    "decapitalize_content(new_cleaned_pages)\n",
    "remove_bullets(new_cleaned_pages)\n",
    "remove_escape(new_cleaned_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge loaded pages with new indexed ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages += new_pages\n",
    "cleaned_pages += new_cleaned_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chunks(pages, \"r\")\n",
    "save_chunks(cleaned_pages, \"c\")\n",
    "\n",
    "# Update processed files list\n",
    "processed_files += [file.replace(folder + \"/\", \"\") for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create several embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "llama1b_embeddings = OllamaEmbeddings(model = \"llama3.2:1b\")\n",
    "llama3b_embeddings = OllamaEmbeddings(model = \"llama3.2:3b\")\n",
    "gemma2b_embeddings = OllamaEmbeddings(model = \"gemma2:2b\")\n",
    "mpnetbase_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "minilm_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "all_embeddings = {\n",
    "    \"llama1b\": OllamaEmbeddings(model = \"llama3.2:1b\"), \n",
    "    \"llama3b\": OllamaEmbeddings(model = \"llama3.2:3b\"),\n",
    "    \"gemma2b\": OllamaEmbeddings(model = \"gemma2:2b\"),\n",
    "    \"mpnetbase\": HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"),\n",
    "    \"minilm\": HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import os\n",
    "\n",
    "embeddings = all_embeddings[\"llama3b\"]\n",
    "vector_store_path = \"embeddings/Llama3.2:3b/raw/Llama3.2:3b_raw\"\n",
    "vector_store = None\n",
    "\n",
    "if os.path.exists(vector_store_path):\n",
    "    vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "\n",
    "else: \n",
    "    vector_store = InMemoryVectorStore.from_documents(documents = pages, embedding = embeddings)\n",
    "    vector_store.dump(vector_store_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
