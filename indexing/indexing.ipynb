{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed chunks in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def save_chunks(pages: list, path: str):\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for chunk in range(len(pages)):\n",
    "        full_path = path + \"/\" + \"chunk_\" + str(chunk + 1)\n",
    "        with open(full_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed chunks (raw, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.load import load\n",
    "\n",
    "def load_chunks(path: str):\n",
    "\n",
    "    pages = []\n",
    "\n",
    "    try:   \n",
    "        for fname in os.listdir(path):\n",
    "            f = os.path.join(path, fname)\n",
    "            with open(f, \"r\") as file:\n",
    "                page = load(json.load(file))\n",
    "                pages.append(page)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alter metadata (this step is document-specific and may change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pikepdf\n",
    "\n",
    "# This document is written in Spanish, so we decide to mark it into metadata\n",
    "pdf = pikepdf.Pdf.open(\"../sources/OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf\",\n",
    "                  allow_overwriting_input = True)\n",
    "\n",
    "edited = False  \n",
    "with pdf.open_metadata() as meta:\n",
    "    if meta[\"keywords\"] == \"\":\n",
    "        meta[\"keywords\"] = \"non-en\"\n",
    "        edited = True\n",
    "        \n",
    "if edited:  \n",
    "    pdf.save(\"../sources/OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all file from source folder to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# processed_files = [\n",
    "#     'BEQ_2301_OVERALL_multi.pdf', \n",
    "#     'CADCAM_BRA_22_Eng.pdf', \n",
    "#     'IOS_Report_FR-IT-ES_rev17.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_LAB_Executive_Summary_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_LAB_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_17_CLI_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_23_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_ITA_19_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_LAB_.pdf'\n",
    "# ]\n",
    "processed_files = []\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        if fname not in processed_files:\n",
    "            files.append(complete_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "chunking_type = \"fixed_number\"\n",
    "chunk_size = 384\n",
    "chunk_overlap = 100\n",
    "semantic_chunking_type = \"gradient\"\n",
    "semantic_chunking_model = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name = semantic_chunking_model)\n",
    "\n",
    "all_chunkings = {\n",
    "    \"page_chunking\": None,\n",
    "    \"fixed_number\": TokenTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap),\n",
    "    \"semantic\": SemanticChunker(embeddings = embeddings, \n",
    "                                breakpoint_threshold_type = semantic_chunking_type)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents and tables within into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "async def split_page_content(files):\n",
    "\n",
    "    docs = []\n",
    "    for file in files:\n",
    "        loader = PyMuPDFLoader(file)\n",
    "        async for doc in loader.alazy_load():\n",
    "            docs.append(doc)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "async def merge_page_content_and_split(files, splitter):\n",
    "\n",
    "    docs = []\n",
    "    page_contents = []\n",
    "    giant_docs = []\n",
    "\n",
    "    for file in files:\n",
    "        loader = PyMuPDFLoader(file)\n",
    "        giant_doc = {\"page_content\": \"\", \"metadata\": \"\"}\n",
    "        first = True\n",
    "        async for doc in loader.alazy_load():\n",
    "            if first:\n",
    "                metadata = {k: v for k, v in doc.metadata.items() if k != \"page\"}\n",
    "                giant_doc[\"metadata\"] = metadata\n",
    "            giant_doc[\"page_content\"] += doc.page_content\n",
    "            first = False\n",
    "        giant_docs.append(giant_doc)\n",
    "\n",
    "    for gdoc in tqdm(giant_docs):\n",
    "        page_contents = splitter.split_text(gdoc[\"page_content\"])\n",
    "        docs += [{\"metadata\": gdoc[\"metadata\"], \"page_content\": pc} for pc in page_contents]\n",
    "\n",
    "    docs = [Document(metadata = doc[\"metadata\"], page_content = doc[\"page_content\"]) for doc in docs]\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "path = \"chunkings/No OCR/\"\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "    path = path + chunking_type + \"/cleaned\" \n",
    "elif chunking_type == \"semantic\":\n",
    "    path = f\"{path}{chunking_type}_{semantic_chunking_type}_minilm_l12\"\n",
    "else:\n",
    "    path = f\"{path}{chunk_size}_{chunk_overlap}\"\n",
    "    \n",
    "pages = load_chunks(path)\n",
    "\n",
    "text_splitter = all_chunkings[chunking_type]\n",
    "\n",
    "# Index new documents  \n",
    "if chunking_type == \"page_chunking\":\n",
    "    new_pages = await split_page_content(files)\n",
    "\n",
    "else:\n",
    "    new_pages = await merge_page_content_and_split(files, text_splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pages))\n",
    "print(len(new_pages))\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process content (text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()\n",
    "\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        if \"non-en\" not in p.metadata[\"keywords\"]:\n",
    "            p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•▪\\-*✔➢●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())\n",
    "\n",
    "\n",
    "remove_non_ASCII(new_pages)\n",
    "decapitalize_content(new_pages)\n",
    "remove_bullets(new_pages)\n",
    "remove_escape(new_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge new pages with existing ones and serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages += new_pages\n",
    "\n",
    "save_chunks(pages, path)\n",
    "\n",
    "# Update processed files list\n",
    "# processed_files += [file.replace(folder + \"/\", \"\") for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "all_embeddings = {\n",
    "    # \"llama3.2:1b\": OllamaEmbeddings(model = \"llama3.2:1b\"),\n",
    "     \n",
    "    # \"llama3.2:3b\": OllamaEmbeddings(model = \"llama3.2:3b\"),\n",
    "     \n",
    "    # \"gemma2b\": OllamaEmbeddings(model = \"llama3.2:1b\"),\n",
    "     \n",
    "    # \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    # \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    # \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    # \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "        model = \"TextEmbedding3LargeDeployment\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        show_progress_bar = True,\n",
    "        chunk_size = 384\n",
    ")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import os\n",
    "\n",
    "\n",
    "path = \"chunkings/No OCR/\"\n",
    "model_name = \"text_embedding_3_large\"\n",
    "\n",
    "if chunking_type == \"page_chunking\":\n",
    "    chunking = chunking_type\n",
    "    path = f\"{path}{chunking_type}/cleaned\"\n",
    "\n",
    "elif chunking_type == \"semantic\":\n",
    "    chunking = f\"{chunking_type}_{semantic_chunking_type}_{model_name}\"\n",
    "    path = f\"{path}{chunking_type}_{semantic_chunking_type}_{model_name}\"\n",
    "    \n",
    "else:\n",
    "    chunking = str(chunk_size) + \"_\" + str(chunk_overlap)\n",
    "    path = f\"{path}{chunk_size}_{chunk_overlap}\"\n",
    "\n",
    "pages = load_chunks(path)\n",
    "\n",
    "embeddings = all_embeddings[model_name]\n",
    "vector_store_path = f\"models/No OCR/{model_name}/{chunking}/{model_name}\"\n",
    "vector_store = None\n",
    "\n",
    "if os.path.exists(vector_store_path):\n",
    "    vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "\n",
    "else: \n",
    "    vector_store = InMemoryVectorStore.from_documents(documents = pages, embedding = embeddings)\n",
    "    vector_store.dump(vector_store_path)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type = \"similarity\", \n",
    "                                      search_kwargs = {\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [\"Intra oral scanner\", \"3D printer\"]\n",
    "producers = [\"Dentsply Sirona\", \"Kavo\", \"3M\", \"GC\", \"Ivoclar\", \"Straumann\", \"Kulzer\", \"Voco\"]\n",
    "intervals = [1, 2, 3, 4]\n",
    "countries = [\"Italy\", \"Germany\", \"Spain\", \"UK\", \"United Kingdom\", \"Brazil\"]\n",
    "\n",
    "all_queries = [\n",
    "    \"Trend of inflation in the dental sector between 2021, 2022, and the first half of 2023\",\n",
    "    \"Dental product brands that offer the best value for money according to dentists\",\n",
    "    \"Which are the most relevant dental brands?\",\n",
    "    \"Which are the most recommended products?\",\n",
    "    \"What are the preferred purchasing channels in different countries?\",\n",
    "    f\"Evolution of {products[1]} adoption\",\n",
    "    f\"Which is the country where {products[0]} is most successful?\",\n",
    "    f\"Evolution of {producers[0]}'s loyalitization capability\",\n",
    "    f\"Evolution of {products[0]}'s market in the last {intervals[2]} years\",\n",
    "    f\"Difference in {products[1]} adoption between {countries[0]} and {countries[4]}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = all_queries[6]\n",
    "\n",
    "docs = vector_store.similarity_search_with_score(query, k = 4)\n",
    "\n",
    "print(\"Query: \" + query)\n",
    "for doc in docs:\n",
    "    print(doc[1])\n",
    "    print(\"Sorgente: \" + doc[0].metadata[\"source\"])\n",
    "    print(doc[0].page_content + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate response from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "llm = OllamaLLM(model = \"llama3.2:3b\")\n",
    "# llm = HuggingFaceEndpoint(huggingfacehub_api_token = HF_TOKEN, \n",
    "#                           repo_id = \"meta-llama/Llama-3.2-3B\",\n",
    "#                           task = \"text-generation\")\n",
    "\n",
    "query = \"Which brand has the highest ratio between the total digital awareness and the unaided awareness?\"\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(query):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on a list of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, variance\n",
    "\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for query in all_queries:\n",
    "    docs_relevances = vector_store.similarity_search_with_score(query, k = 4)\n",
    "    all_scores.append(docs_relevances[0][1])\n",
    "\n",
    "avg = mean(all_scores)\n",
    "var = variance(all_scores, avg)\n",
    "\n",
    "print(f\"{avg:.3f}, {var:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute number of different retrieved chunks between two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "model_name_1 = \"minilm-l6\"\n",
    "model_name_2 = \"minilm-l12\"\n",
    "embeddings_1 = all_embeddings[model_name_1][\"model\"]\n",
    "embeddings_2 = all_embeddings[model_name_2][\"model\"]\n",
    "\n",
    "vector_store_path_1 = all_embeddings[model_name_1][\"cleaned-path\"]\n",
    "vector_store_path_2 = all_embeddings[model_name_2][\"cleaned-path\"]\n",
    "\n",
    "vector_store_1 = InMemoryVectorStore.load(path = vector_store_path_1, embedding = embeddings_1)\n",
    "vector_store_2 = InMemoryVectorStore.load(path = vector_store_path_2, embedding = embeddings_2)\n",
    "\n",
    "diff = 0\n",
    "\n",
    "for query in all_queries:\n",
    "\n",
    "    chunks_1 = vector_store_1.similarity_search(query, k = 4)\n",
    "    chunks_2 = vector_store_2.similarity_search(query, k = 4)\n",
    "\n",
    "    chunks_mapped_1 = list(map(lambda c: (c.metadata[\"source\"], c.metadata[\"page\"]), chunks_1))\n",
    "    chunks_mapped_2 = list(map(lambda c: (c.metadata[\"source\"], c.metadata[\"page\"]), chunks_2))\n",
    "\n",
    "    diff += sum([1 for tuple in chunks_mapped_1 if tuple not in chunks_mapped_2])\n",
    "\n",
    "print(diff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
