{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case document content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes non ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes bulleted and numbered lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•\\-*✔●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes multiple consecutive escape characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed chunks in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "\n",
    "def save_chunks(pages: list, type: str):\n",
    "\n",
    "    \"\"\"Saves on disk each processed chunk (raw or cleaned), given a flag\n",
    "    * type = \"r\" -> raw chunks\n",
    "    * type = \"c\" -> cleaned chunks \"\"\"\n",
    "\n",
    "    path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    full_path = \"\"\n",
    "\n",
    "    match type:\n",
    "        case \"r\":\n",
    "            full_path = path + \"/raw/chunk_\"\n",
    "        case \"c\":\n",
    "            full_path = path + \"/cleaned/chunk_\"\n",
    "        case _:\n",
    "            return\n",
    "\n",
    "    for chunk in range(len(pages)):\n",
    "        current_path = full_path + str(chunk + 1)\n",
    "        with open(current_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed chunks (raw, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.load import load\n",
    "\n",
    "def load_chunks(type: str = \"r\"):\n",
    "\n",
    "    \"\"\"Retrieves all processed (raw or cleaned) chunks, given a flag\n",
    "    * type = \"r\" -> raw chunks\n",
    "    * type = \"c\" -> cleaned chunks \"\"\"\n",
    "\n",
    "    path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    full_path = \"\"\n",
    "    pages = []\n",
    "\n",
    "    match type:\n",
    "        case \"r\": \n",
    "            full_path = path + \"/raw\"\n",
    "        case \"c\":\n",
    "            full_path = path + \"/cleaned\"\n",
    "        case _:\n",
    "            return None\n",
    "            \n",
    "    for fname in os.listdir(full_path):\n",
    "        f = os.path.join(full_path, fname)\n",
    "        with open(f, \"r\") as file:\n",
    "            page = load(json.load(file))\n",
    "            pages.append(page)\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all file from source folder to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ENABLE_LOAD = False\n",
    "# processed_files = [\n",
    "#     'BEQ_2301_OVERALL_multi.pdf', \n",
    "#     'CADCAM_BRA_22_Eng.pdf', \n",
    "#     'IOS_Report_FR-IT-ES_rev17.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_LAB_Executive_Summary_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_LAB_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_17_CLI_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_23_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_ITA_19_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_LAB_.pdf'\n",
    "# ]\n",
    "processed_files = []\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        if fname not in processed_files or not ENABLE_LOAD:\n",
    "            files.append(complete_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents and tables within into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import json\n",
    "from langchain_core.load import load\n",
    "\n",
    "pages = []\n",
    "cleaned_pages = []\n",
    "\n",
    "# path_raw = \"parsed_documents/PyMuPDFLoader - No OCR/raw\"\n",
    "# path_cleaned = \"parsed_documents/PyMuPDFLoader - No OCR/cleaned\"\n",
    "\n",
    "# Retrieve processed raw chunks\n",
    "# for fname in os.listdir(path_raw):\n",
    "#     f = os.path.join(path_raw, fname)\n",
    "#     with open(f, \"r\") as file:\n",
    "#         page_ = json.load(file)\n",
    "#         page = load(page_)\n",
    "#         pages.append(page)\n",
    "\n",
    "# and cleaned chunks\n",
    "# for fname in os.listdir(path_cleaned):\n",
    "#     f = os.path.join(path_cleaned, fname)\n",
    "#     with open(f, \"r\") as file:\n",
    "#         page_ = json.load(file)\n",
    "#         page = load(page_)\n",
    "#         pages.append(page)\n",
    "\n",
    "pages = load_chunks(\"r\")\n",
    "cleaned_pages = load_chunks(\"c\")\n",
    "\n",
    "# Index new documents  \n",
    "new_pages = []\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file, extract_images = False)\n",
    "    async for page in loader.alazy_load():\n",
    "        new_pages.append(page)\n",
    "\n",
    "# Update processed files list\n",
    "for file in files:\n",
    "    processed_files.append(file.replace(folder + \"/\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process content (text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "new_cleaned_pages = copy.deepcopy(new_pages)\n",
    "\n",
    "remove_non_ASCII(new_cleaned_pages)\n",
    "decapitalize_content(new_cleaned_pages)\n",
    "remove_bullets(new_cleaned_pages)\n",
    "remove_escape(new_cleaned_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge loaded pages with new indexed ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages += new_pages\n",
    "cleaned_pages += new_cleaned_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "\n",
    "# path_raw = \"parsed_documents/PyMuPDFLoader - No OCR/raw/chunk_\"\n",
    "# path_cleaned = \"parsed_documents/PyMuPDFLoader - No OCR/cleaned/chunk_\"\n",
    "\n",
    "# for chunk in range(len(pages)):\n",
    "\n",
    "#     current_path_raw = path_raw + str(chunk + 1)\n",
    "#     current_path_cleaned = path_cleaned + str(chunk + 1)\n",
    "#     page = pages[chunk]\n",
    "\n",
    "#     with open(current_path_raw, \"w\") as ser_file:\n",
    "#         page_d = dumpd(pages[chunk])\n",
    "#         json.dump(page_d, ser_file)\n",
    "\n",
    "#     with open(current_path_cleaned, \"w\") as ser_file:\n",
    "#         page_d = dumpd(cleaned_pages[chunk])\n",
    "#         json.dump(page_d, ser_file)\n",
    "\n",
    "save_chunks(pages, \"r\")\n",
    "save_chunks(cleaned_pages, \"c\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
