{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case document content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes non ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes bulleted and numbered lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•\\-*✔●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes multiple consecutive escape characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed chunks in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "\n",
    "def save_chunks(pages: list, type: str):\n",
    "\n",
    "    \"\"\"Saves on disk each processed chunk (raw or cleaned), given a flag\n",
    "    * type = \"r\" -> raw chunks\n",
    "    * type = \"c\" -> cleaned chunks \"\"\"\n",
    "\n",
    "    path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    full_path = \"\"\n",
    "\n",
    "    match type:\n",
    "        case \"r\":\n",
    "            full_path = path + \"/raw/chunk_\"\n",
    "        case \"c\":\n",
    "            full_path = path + \"/cleaned/chunk_\"\n",
    "        case _:\n",
    "            return\n",
    "\n",
    "    for chunk in range(len(pages)):\n",
    "        current_path = full_path + str(chunk + 1)\n",
    "        with open(current_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed chunks (raw, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_core.load import load\n",
    "\n",
    "def load_chunks(type: str = \"r\"):\n",
    "\n",
    "    \"\"\"Retrieves all processed (raw or cleaned) chunks, given a flag\n",
    "    * type = \"r\" -> raw chunks\n",
    "    * type = \"c\" -> cleaned chunks \"\"\"\n",
    "\n",
    "    path = \"parsed_documents/PyMuPDFLoader - No OCR\"\n",
    "    full_path = \"\"\n",
    "    pages = []\n",
    "\n",
    "    match type:\n",
    "        case \"r\": \n",
    "            full_path = path + \"/raw\"\n",
    "        case \"c\":\n",
    "            full_path = path + \"/cleaned\"\n",
    "        case _:\n",
    "            return None\n",
    "            \n",
    "    for fname in os.listdir(full_path):\n",
    "        f = os.path.join(full_path, fname)\n",
    "        with open(f, \"r\") as file:\n",
    "            page = load(json.load(file))\n",
    "            pages.append(page)\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all file from source folder to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "processed_files = [\n",
    "    'BEQ_2301_OVERALL_multi.pdf', \n",
    "    'CADCAM_BRA_22_Eng.pdf', \n",
    "    'IOS_Report_FR-IT-ES_rev17.pdf', \n",
    "    'OMNI_DIGITAL_EU_15_CLI_.pdf', \n",
    "    'OMNI_DIGITAL_EU_15_CLI_LAB_Executive_Summary_.pdf', \n",
    "    'OMNI_DIGITAL_EU_15_LAB_.pdf', \n",
    "    'OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf', \n",
    "    'OMNI-DIGITAL_ITA_17_CLI_.pdf', \n",
    "    'OMNI-DIGITAL_ITA_23_CLI_.pdf', \n",
    "    'OMNI_DIGITAL_ITA_19_CLI_LAB_INTEGRATED_.pdf', \n",
    "    'OMNI_DIGITAL_SPA_19_CLI_.pdf', \n",
    "    'OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf', \n",
    "    'OMNI_DIGITAL_SPA_19_LAB_.pdf'\n",
    "]\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        if fname not in processed_files:\n",
    "            files.append(complete_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents and tables within into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7523/3008655791.py:26: LangChainBetaWarning: The function `load` is in beta. It is actively being worked on, so the API may change.\n",
      "  page = load(json.load(file))\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pages = load_chunks(\"r\")\n",
    "cleaned_pages = load_chunks(\"c\")\n",
    "\n",
    "# Index new documents  \n",
    "new_pages = []\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file, extract_images = False)\n",
    "    async for page in loader.alazy_load():\n",
    "        new_pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process content (text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "new_cleaned_pages = copy.deepcopy(new_pages)\n",
    "\n",
    "remove_non_ASCII(new_cleaned_pages)\n",
    "decapitalize_content(new_cleaned_pages)\n",
    "remove_bullets(new_cleaned_pages)\n",
    "remove_escape(new_cleaned_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge loaded pages with new indexed ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages += new_pages\n",
    "cleaned_pages += new_cleaned_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chunks(pages, \"r\")\n",
    "save_chunks(cleaned_pages, \"c\")\n",
    "\n",
    "# Update processed files list\n",
    "processed_files += [file.replace(folder + \"/\", \"\") for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create several embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "all_embeddings = {\n",
    "    \"llama1b\": \n",
    "    {\n",
    "        \"model\": OllamaEmbeddings(model = \"llama3.2:1b\"),\n",
    "        \"raw_path\": \"embeddings/llama3.2:1b/raw/llama3.2:1b_raw\",\n",
    "        \"cleaned_path\": \"embeddings/llama3.2:1b/cleaned/llama3.2:1b_cleaned\"\n",
    "    }, \n",
    "    \"llama3b\": \n",
    "    {\n",
    "        \"model\": OllamaEmbeddings(model = \"llama3.2:3b\"),\n",
    "        \"raw_path\": \"embeddings/llama3.2:3b/raw/llama3.2:3b_raw\",\n",
    "        \"cleaned_path\": \"embeddings/llama3.2:3b/cleaned/llama3.2:3b_cleaned\"\n",
    "    },\n",
    "    \"gemma2b\": \n",
    "    {\n",
    "        \"model\": OllamaEmbeddings(model = \"gemma2:2b\"),\n",
    "        \"raw_path\": \"embeddings/gemma2b/raw/gemma2b_raw\",\n",
    "        \"cleaned_path\": \"embeddings/gemma2b/cleaned/gemma2b_cleaned\"\n",
    "    },\n",
    "    \"mpnetbase\": \n",
    "    {\n",
    "        \"model\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "        \"raw_path\": \"embeddings/mpnet_base_v2/raw/mpnet_base_v2_raw\",\n",
    "        \"cleaned_path\": \"embeddings/mpnet_base_v2/cleaned/mpnet_base_v2_cleaend\"\n",
    "    },\n",
    "    \"minilm\": \n",
    "    {\n",
    "        \"model\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-l6-v2\"),\n",
    "        \"raw_path\": \"embeddings/minilm/raw/minilm_raw\",\n",
    "        \"cleaned_path\": \"embeddings/minilm/cleaned/minilm_cleaned\"\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import os\n",
    "\n",
    "embeddings = all_embeddings[\"llama1b\"][\"model\"]\n",
    "vector_store_path = all_embeddings[\"llama1b\"][\"raw_path\"]\n",
    "vector_store = None\n",
    "\n",
    "if os.path.exists(vector_store_path):\n",
    "    vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "\n",
    "else: \n",
    "    vector_store = InMemoryVectorStore.from_documents(documents = cleaned_pages, embedding = embeddings)\n",
    "    vector_store.dump(vector_store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = [\"Scanner intra orale\", \"Stampante 3D\", \"Fresatura chairside\", \"Apple Vision Pro\"]\n",
    "producers = [\"Dentsply Sirona\", \"Kavo\", \"3M\", \"GC\", \"Ivoclar\", \"Straumann\", \"Kulzer\", \"Voco\", \"Dentfix\"]\n",
    "intervals = [1, 2, 3, 4, 5, 10]\n",
    "countries = [\"Italia\", \"Germania\", \"Spagna\", \"UK\", \"Regno Unito\", \"Brasile\", \"Costa d'avorio\", \"Corea del Nord\"]\n",
    "\n",
    "queries = [\n",
    "    f\"Evoluzione del mercato del {products[0]} negli ultimi {intervals[0]} anni\"\n",
    "    f\"Evoluzione della capacità di fidelizzazione di {producers[0]}\",\n",
    "    \"Andamento dell’inflazione nel settore dentale tra il 2021, il 2022 e il primo semestre 2023\",\n",
    "    \"Brand di prodotti dentali che offrono il miglior rapporto qualità-prezzo secondo i dentisti\",\n",
    "    \"Quali sono i brand dentali più rilevanti?\",\n",
    "    \"Quali sono i prodotti più raccomandati?\",\n",
    "    f\"Quali sono i canali di acquisto preferiti nelle varie nazioni?\",\n",
    "    f\"Evoluzione dell'adozione di {products[0]}\"\n",
    "    f\"Differenza nell'adozione di {products[0]} tra {countries[0]} e {countries[1]}\",\n",
    "    f\"Qual è il Paese in cui {producers[0]} ha più successo?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
