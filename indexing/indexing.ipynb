{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower case document content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes non ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes bulleted and numbered lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•\\-*✔●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removes multiple consecutive escape characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed chunks (raw, cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks(type: str = \"r\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all file from source folder to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ENABLE_LOAD = False\n",
    "# processed_files = [\n",
    "#     'BEQ_2301_OVERALL_multi.pdf', \n",
    "#     'CADCAM_BRA_22_Eng.pdf', \n",
    "#     'IOS_Report_FR-IT-ES_rev17.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_CLI_LAB_Executive_Summary_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_15_LAB_.pdf', \n",
    "#     'OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_17_CLI_.pdf', \n",
    "#     'OMNI-DIGITAL_ITA_23_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_ITA_19_CLI_LAB_INTEGRATED_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf', \n",
    "#     'OMNI_DIGITAL_SPA_19_LAB_.pdf'\n",
    "# ]\n",
    "processed_files = []\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        if fname not in processed_files or not ENABLE_LOAD:\n",
    "            files.append(complete_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse documents and tables within into pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import json\n",
    "from langchain_core.load import load\n",
    "\n",
    "pages = []\n",
    "cleaned_pages = []\n",
    "\n",
    "if ENABLE_LOAD:\n",
    "    \n",
    "    path_raw = \"parsed_documents/PyMuPDFLoader - No OCR/raw\"\n",
    "    path_cleaned = \"parsed_documents/PyMuPDFLoader - No OCR/cleaned\"\n",
    "\n",
    "    # Retrieve processed raw chunks\n",
    "    for fname in os.listdir(path_raw):\n",
    "        f = os.path.join(path_raw, fname)\n",
    "        with open(f, \"r\") as file:\n",
    "            page_ = json.load(file)\n",
    "            page = load(page_)\n",
    "            pages.append(page)\n",
    "\n",
    "    # and cleaned chunks\n",
    "    for fname in os.listdir(path_cleaned):\n",
    "        f = os.path.join(path_cleaned, fname)\n",
    "        with open(f, \"r\") as file:\n",
    "            page_ = json.load(file)\n",
    "            page = load(page_)\n",
    "            pages.append(page)\n",
    "\n",
    "\n",
    "# Index new documents  \n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file, extract_images = False)\n",
    "    async for page in loader.alazy_load():\n",
    "        pages.append(page)\n",
    "\n",
    "print(len(pages))\n",
    "\n",
    "# Update processed files list\n",
    "for file in files:\n",
    "    processed_files.append(file.replace(folder + \"/\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process content (text cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "cleaned_pages = copy.deepcopy(pages)\n",
    "\n",
    "remove_non_ASCII(cleaned_pages)\n",
    "decapitalize_content(cleaned_pages)\n",
    "remove_bullets(cleaned_pages)\n",
    "remove_escape(cleaned_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumpd\n",
    "import json\n",
    "\n",
    "path_raw = \"parsed_documents/PyMuPDFLoader - No OCR/raw/chunk_\"\n",
    "path_cleaned = \"parsed_documents/PyMuPDFLoader - No OCR/cleaned/chunk_\"\n",
    "\n",
    "for chunk in range(len(pages)):\n",
    "\n",
    "    current_path_raw = path_raw + str(chunk + 1)\n",
    "    current_path_cleaned = path_cleaned + str(chunk + 1)\n",
    "    page = pages[chunk]\n",
    "\n",
    "    with open(current_path_raw, \"w\") as ser_file:\n",
    "        page_d = dumpd(pages[chunk])\n",
    "        json.dump(page_d, ser_file)\n",
    "\n",
    "    with open(current_path_cleaned, \"w\") as ser_file:\n",
    "        page_d = dumpd(cleaned_pages[chunk])\n",
    "        json.dump(page_d, ser_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse content with Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "loader = UnstructuredLoader(\n",
    "    file_path = \"../sources/BEQ_2301_OVERALL_multi.pdf\", \n",
    "    strategy = \"hi_res\")\n",
    "docs = []\n",
    "async for doc in loader.alazy_load():\n",
    "    docs.append(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
