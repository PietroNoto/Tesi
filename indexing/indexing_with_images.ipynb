{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all image descriptions from ChromaDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "\n",
    "\n",
    "client = chromadb.PersistentClient(path = \"chromadb\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name = \"my_collection\", metadata = {\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "image_descriptions = collection.get(where = {\"type\": \"image\"})[\"documents\"]\n",
    "ids = collection.get(where = {\"type\": \"image\"})[\"ids\"]\n",
    "ids = [id.split(\"_page_\") for id in ids]\n",
    "ids = [{\"file\": id[0], \"page\": int(id[-1].split(\"_image_\")[0])} for id in ids]\n",
    "\n",
    "for j in range(len(ids)):\n",
    "    ids[j][\"image_description\"] = image_descriptions[j]\n",
    "\n",
    "with open(\"image_descriptions/image_descriptions.json\", \"w\") as f:\n",
    "    json.dump(ids, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents into pages, with text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        files.append(complete_path)\n",
    "\n",
    "docs = []\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    async for doc in loader.alazy_load():\n",
    "        docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove first pages and index pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc for doc in docs if doc.metadata[\"page\"] != 0]\n",
    "\n",
    "docs = [doc for doc in docs \n",
    "        if not doc.page_content.lower().startswith((\"index\", \"table of contents\", \"índice\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate text and image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"image_descriptions/image_descriptions.json\", \"r\") as f:\n",
    "    image_descriptions = json.load(f)\n",
    "\n",
    "    for imd in image_descriptions:\n",
    "        file = f\"../sources/{imd[\"file\"]}.pdf\"\n",
    "        page = imd[\"page\"]\n",
    "        doc = next(filter(lambda doc: doc.metadata[\"source\"] == file and doc.metadata[\"page\"] == page, docs), None)\n",
    "        if doc != None:\n",
    "            doc.page_content += f\"\\n{imd[\"image_description\"]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()\n",
    "\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        if \"non-en\" not in p.metadata[\"keywords\"]:\n",
    "            p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•▪\\-*✔➢●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())\n",
    "\n",
    "\n",
    "remove_non_ASCII(docs)\n",
    "decapitalize_content(docs)\n",
    "remove_bullets(docs)\n",
    "remove_escape(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_and_split(docs: list[Document], splitter):\n",
    "\n",
    "    from collections import defaultdict\n",
    "    \n",
    "\n",
    "    docs_groups = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        docs_groups[doc.metadata[\"source\"]].append(doc)\n",
    "\n",
    "    giant_docs = []\n",
    "    for _, docs in docs_groups.items():\n",
    "        giant_doc = {}\n",
    "        metadata = {k: v for k, v in docs[0].metadata.items() if k != \"page\"}\n",
    "        page_content = \"\"\n",
    "        for doc in docs:\n",
    "            page_content += doc.page_content\n",
    "        giant_doc[\"metadata\"] = metadata\n",
    "        giant_doc[\"page_content\"] = page_content\n",
    "        giant_docs.append(giant_doc)\n",
    "\n",
    "    files = []\n",
    "    for gdoc in giant_docs:\n",
    "        page_contents = splitter.split_text(gdoc[\"page_content\"])\n",
    "        files += [{\"metadata\": gdoc[\"metadata\"], \"page_content\": pc} for pc in page_contents]\n",
    "\n",
    "    files = [Document(metadata = file[\"metadata\"], page_content = file[\"page_content\"]) for file in files]\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def save_chunks(pages: list, path: str):\n",
    "\n",
    "    from langchain_core.load import dumpd\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for chunk in range(len(pages)):\n",
    "        full_path = path + \"/\" + \"chunk_\" + str(chunk + 1)\n",
    "        with open(full_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)\n",
    "\n",
    "\n",
    "chunk_types = [\"page_chunking\", \"fixed_number\"]\n",
    "chunk_sizes = [256, 384]\n",
    "chunk_overlaps = [0, 20, 50, 100]\n",
    "base_path = \"chunkings/Text+Images/\"\n",
    "\n",
    "for chunk_type in chunk_types:\n",
    "    if chunk_type == \"fixed_number\":\n",
    "        for chunk_size in chunk_sizes:\n",
    "            for chunk_overlap in chunk_overlaps:\n",
    "                splitter = TokenTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap)\n",
    "                split_docs = merge_and_split(docs, splitter)\n",
    "                path = f\"{base_path}/{chunk_size}_{chunk_overlap}\"\n",
    "                os.mkdir(path)\n",
    "                save_chunks(split_docs, path)\n",
    "    else:\n",
    "        path = f\"{base_path}/page_chunking\"\n",
    "        os.mkdir(path)\n",
    "        save_chunks(docs, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "all_embeddings = {     \n",
    "    \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    # \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "    #     azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "    #     api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    #     model = \"TextEmbedding3LargeDeployment\",\n",
    "    #     api_version = \"2023-05-15\",\n",
    "    #     show_progress_bar = True,\n",
    "    #     chunk_size = 128\n",
    "    # )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import os\n",
    "\n",
    "\n",
    "def load_chunks(path: str):\n",
    "\n",
    "    import os\n",
    "    import json\n",
    "    from langchain_core.load import load\n",
    "\n",
    "\n",
    "    pages = []\n",
    "\n",
    "    try:   \n",
    "        for fname in os.listdir(path):\n",
    "            f = os.path.join(path, fname)\n",
    "            with open(f, \"r\") as file:\n",
    "                page = load(json.load(file))\n",
    "                pages.append(page)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "    \n",
    "    return pages\n",
    "\n",
    "\n",
    "model_name = \"text_embedding_3_large\"\n",
    "chunk_type = \"fixed_number\"\n",
    "chunk_size = 256\n",
    "chunk_overlap = 100\n",
    "base_path = \"chunkings/Text+Images\"\n",
    "\n",
    "for model_name in all_embeddings:\n",
    "    for chunk_type in [\"page_chunking\", \"fixed_number\"]:\n",
    "        for chunk_size in [256, 384]:\n",
    "            for chunk_overlap in [0, 20, 50, 100]:\n",
    "\n",
    "                if chunk_type == \"page_chunking\":\n",
    "                    chunking = chunk_type\n",
    "\n",
    "                elif chunk_type == \"fixed_number\":\n",
    "                    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "                    \n",
    "                path = f\"{base_path}/{chunking}\"\n",
    "                docs = load_chunks(path)\n",
    "\n",
    "                embeddings = all_embeddings[model_name]\n",
    "                vector_store_path = f\"models/Text+Images/{model_name}/{chunking}/{chunking}_{model_name}\"\n",
    "\n",
    "                if os.path.exists(vector_store_path):\n",
    "                    vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "\n",
    "                else: \n",
    "                    vector_store = InMemoryVectorStore.from_documents(documents = docs, embedding = embeddings)\n",
    "                    vector_store.dump(vector_store_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/Text+Images/minilm_l6/256_100/256_100_minilm_l6\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m InMemoryVectorStore\u001b[38;5;241m.\u001b[39mload(path \u001b[38;5;241m=\u001b[39m path, embedding \u001b[38;5;241m=\u001b[39m all_embeddings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminilm_l6\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Bego?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<stringsource>:69\u001b[0m, in \u001b[0;36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1428\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._line_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1470\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1277\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1905\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2197\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2194\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2197\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2199\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2266\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2263\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2264\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2266\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2267\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/my_pyenv/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/my_pyenv/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"models/Text+Images/minilm_l6/256_100/256_100_minilm_l6\"\n",
    "vector_store = InMemoryVectorStore.load(path = path, embedding = all_embeddings[\"minilm_l6\"])\n",
    "docs = vector_store.similarity_search(\"What is Bego?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
