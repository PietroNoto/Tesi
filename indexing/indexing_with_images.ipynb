{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all image descriptions from ChromaDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import json\n",
    "\n",
    "\n",
    "client = chromadb.PersistentClient(path = \"chromadb\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name = \"my_collection\", metadata = {\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "image_descriptions = collection.get(where = {\"type\": \"image\"})[\"documents\"]\n",
    "ids = collection.get(where = {\"type\": \"image\"})[\"ids\"]\n",
    "ids = [id.split(\"_page_\") for id in ids]\n",
    "ids = [{\"file\": id[0], \"page\": int(id[-1].split(\"_image_\")[0])} for id in ids]\n",
    "\n",
    "for j in range(len(ids)):\n",
    "    ids[j][\"image_description\"] = image_descriptions[j]\n",
    "\n",
    "with open(\"image_descriptions/image_descriptions.json\", \"w\") as f:\n",
    "    json.dump(ids, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents into pages, with text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        files.append(complete_path)\n",
    "\n",
    "docs = []\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    async for doc in loader.alazy_load():\n",
    "        docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove first page and index pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc for doc in docs if doc.metadata[\"page\"] != 0]\n",
    "\n",
    "docs = [doc for doc in docs \n",
    "        if not doc.page_content.lower().startswith((\"index\", \"table of contents\", \"índice\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate text and image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"image_descriptions/image_descriptions.json\", \"r\") as f:\n",
    "    image_descriptions = json.load(f)\n",
    "\n",
    "    for imd in image_descriptions:\n",
    "        file = f\"../sources/{imd[\"file\"]}.pdf\"\n",
    "        page = imd[\"page\"]\n",
    "        doc = next(filter(lambda doc: doc.metadata[\"source\"] == file and doc.metadata[\"page\"] == page, docs), None)\n",
    "        if doc != None:\n",
    "            doc.page_content += f\"\\n{imd[\"image_description\"]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()\n",
    "\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        if \"non-en\" not in p.metadata[\"keywords\"]:\n",
    "            p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•▪\\-*✔➢●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())\n",
    "\n",
    "\n",
    "remove_non_ASCII(docs)\n",
    "decapitalize_content(docs)\n",
    "remove_bullets(docs)\n",
    "remove_escape(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_and_split(docs: list[Document], splitter):\n",
    "\n",
    "    from collections import defaultdict\n",
    "    \n",
    "\n",
    "    docs_groups = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        docs_groups[doc.metadata[\"source\"]].append(doc)\n",
    "\n",
    "    giant_docs = []\n",
    "    for _, docs in docs_groups.items():\n",
    "        giant_doc = {}\n",
    "        metadata = {k: v for k, v in docs[0].metadata.items() if k != \"page\"}\n",
    "        page_content = \"\"\n",
    "        for doc in docs:\n",
    "            page_content += doc.page_content\n",
    "        giant_doc[\"metadata\"] = metadata\n",
    "        giant_doc[\"page_content\"] = page_content\n",
    "        giant_docs.append(giant_doc)\n",
    "\n",
    "    files = []\n",
    "    for gdoc in giant_docs:\n",
    "        page_contents = splitter.split_text(gdoc[\"page_content\"])\n",
    "        files += [{\"metadata\": gdoc[\"metadata\"], \"page_content\": pc} for pc in page_contents]\n",
    "\n",
    "    files = [Document(metadata = file[\"metadata\"], page_content = file[\"page_content\"]) for file in files]\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def save_chunks(pages: list, path: str):\n",
    "\n",
    "    from langchain_core.load import dumpd\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for chunk in range(len(pages)):\n",
    "        full_path = path + \"/\" + \"chunk_\" + str(chunk + 1)\n",
    "        with open(full_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)\n",
    "\n",
    "\n",
    "chunk_types = [\"page_chunking\", \"fixed_number\"]\n",
    "chunk_sizes = [256, 384]\n",
    "chunk_overlaps = [0, 20, 50, 100]\n",
    "base_path = \"chunkings/Text+Images/\"\n",
    "\n",
    "for chunk_type in chunk_types:\n",
    "    if chunk_type == \"fixed_number\":\n",
    "        for chunk_size in chunk_sizes:\n",
    "            for chunk_overlap in chunk_overlaps:\n",
    "                splitter = TokenTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap)\n",
    "                split_docs = merge_and_split(docs, splitter)\n",
    "                path = f\"{base_path}/{chunk_size}_{chunk_overlap}\"\n",
    "                os.mkdir(path)\n",
    "                save_chunks(split_docs, path)\n",
    "    else:\n",
    "        path = f\"{base_path}/page_chunking\"\n",
    "        os.mkdir(path)\n",
    "        save_chunks(docs, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "all_embeddings = {     \n",
    "    \"mpnet_base_v2\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\"),\n",
    "     \n",
    "    \"minilm_l6\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "    \n",
    "    \"minilm_l12\": HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L12-v2\"),\n",
    "\n",
    "    \"multilingual\": HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"),\n",
    "    \n",
    "    \"text_embedding_3_large\": AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=\"https://keystone1.openai.azure.com/openai/deployments/text-embedding-3-large-2/embeddings?api-version=2023-05-15\",\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "        model = \"TextEmbedding3LargeDeployment\",\n",
    "        api_version = \"2023-05-15\",\n",
    "        show_progress_bar = True,\n",
    "        chunk_size = 128\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import os\n",
    "\n",
    "\n",
    "def load_chunks(path: str):\n",
    "\n",
    "    import os\n",
    "    import json\n",
    "    from langchain_core.load import load\n",
    "\n",
    "\n",
    "    pages = []\n",
    "\n",
    "    try:   \n",
    "        for fname in os.listdir(path):\n",
    "            f = os.path.join(path, fname)\n",
    "            with open(f, \"r\") as file:\n",
    "                page = load(json.load(file))\n",
    "                pages.append(page)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "    \n",
    "    return pages\n",
    "\n",
    "\n",
    "model_name = \"text_embedding_3_large\"\n",
    "chunk_type = \"fixed_number\"\n",
    "chunk_size = 256\n",
    "chunk_overlap = 100\n",
    "base_path = \"chunkings/Text+Images\"\n",
    "\n",
    "if chunk_type == \"page_chunking\":\n",
    "    chunking = chunk_type\n",
    "    path = f\"{base_path}/{chunk_type}\"\n",
    "\n",
    "elif chunk_type == \"fixed_number\":\n",
    "    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "    path = f\"{base_path}/{chunking}\"\n",
    "\n",
    "docs = load_chunks(path)\n",
    "\n",
    "embeddings = all_embeddings[model_name]\n",
    "vector_store_path = f\"models/Text+Images/{model_name}/{chunking}/{chunking}_{model_name}\"\n",
    "\n",
    "if os.path.exists(vector_store_path):\n",
    "    vector_store = InMemoryVectorStore.load(path = vector_store_path, embedding = embeddings)\n",
    "\n",
    "else: \n",
    "    vector_store = InMemoryVectorStore.from_documents(documents = docs, embedding = embeddings)\n",
    "    vector_store.dump(vector_store_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
