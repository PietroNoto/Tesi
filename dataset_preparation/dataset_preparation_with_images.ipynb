{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents into pages, with text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        files.append(complete_path)\n",
    "\n",
    "docs = []\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    async for doc in loader.alazy_load():\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove first pages and index pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc for doc in docs if doc.metadata[\"page\"] != 0]\n",
    "\n",
    "docs = [doc for doc in docs \n",
    "        if not doc.page_content.lower().startswith((\"index\", \"table of contents\", \"índice\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate text with image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"../indexing/image_descriptions/image_descriptions.json\", \"r\") as f:\n",
    "    image_descriptions = json.load(f)\n",
    "\n",
    "    for imd in image_descriptions:\n",
    "        file = f\"../sources/{imd[\"file\"]}.pdf\"\n",
    "        page = imd[\"page\"]\n",
    "        doc = next(filter(lambda doc: doc.metadata[\"source\"] == file and doc.metadata[\"page\"] == page, docs), None)\n",
    "        if doc != None:\n",
    "            doc.page_content += f\"\\n{imd[\"image_description\"]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()\n",
    "\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        if \"non-en\" not in p.metadata[\"keywords\"]:\n",
    "            p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•▪\\-*✔➢●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())\n",
    "\n",
    "\n",
    "remove_non_ASCII(docs)\n",
    "decapitalize_content(docs)\n",
    "remove_bullets(docs)\n",
    "remove_escape(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_and_split(docs: list[Document], splitter):\n",
    "\n",
    "    from collections import defaultdict\n",
    "    \n",
    "\n",
    "    docs_groups = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        docs_groups[doc.metadata[\"source\"]].append(doc)\n",
    "\n",
    "    giant_docs = []\n",
    "    for _, docs in docs_groups.items():\n",
    "        giant_doc = {}\n",
    "        metadata = {k: v for k, v in docs[0].metadata.items() if k != \"page\"}\n",
    "        page_content = \"\"\n",
    "        for doc in docs:\n",
    "            page_content += doc.page_content\n",
    "        giant_doc[\"metadata\"] = metadata\n",
    "        giant_doc[\"page_content\"] = page_content\n",
    "        giant_docs.append(giant_doc)\n",
    "\n",
    "    files = []\n",
    "    for gdoc in giant_docs:\n",
    "        page_contents = splitter.split_text(gdoc[\"page_content\"])\n",
    "        files += [{\"metadata\": gdoc[\"metadata\"], \"page_content\": pc} for pc in page_contents]\n",
    "\n",
    "    files = [Document(metadata = file[\"metadata\"], page_content = file[\"page_content\"]) for file in files]\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def save_chunks(pages: list, path: str):\n",
    "\n",
    "    from langchain_core.load import dumpd\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for chunk in range(len(pages)):\n",
    "        full_path = path + \"/\" + \"chunk_\" + str(chunk + 1)\n",
    "        with open(full_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)\n",
    "\n",
    "\n",
    "chunk_size = 2_000\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_size/10,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "docs = merge_and_split(docs, text_splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup agent for QA generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o\",\n",
    "    max_tokens = 256\n",
    ")\n",
    "\n",
    "path = \"../indexing/models/Text+Images/multilingual/256_20/256_20_multilingual\"\n",
    "vector_store = InMemoryVectorStore.load(path = path, \n",
    "                                        embedding = HuggingFaceEmbeddings(model_name = \"intfloat/multilingual-e5-large\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt_1 = \"\"\"\n",
    "Your task is to write a question and an answer given a context, country, year and, optionally, a target, which can be clinic (dental practise) and/or lab (laboratory).\n",
    "Your question should be and answerable with a one-sentenced factual answer from the context.\n",
    "This means that your question MUST NOT mention something like \"according to the passage/map/picture/graph/chart/line\" or \"context\".\n",
    "Identify, if any, a country from the context and refer to it when you formulate the question, otherwise use the country provided.\n",
    "Alongside the country, enrich the question with the year provided, for example: \"Which is the top brand in France in 2023?\"\n",
    "Additionally, if the target is either clinic or lab (but not both) you should mention it in your question.\n",
    "All words of both the answer and the question must be in english.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Question: (your question)\n",
    "Answer: (your answer to the question)\n",
    "\n",
    "Here is the context, country, year and target.\n",
    "\n",
    "Context: {context}\\n\n",
    "Country: {country}\\n\n",
    "Year: {year}\\n\n",
    "Target: {target}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt_2 = \"\"\"\n",
    "Your task is to write a question and an answer given a context, two vars and a constant.\n",
    "Your question should be and answerable with a one-sentenced factual answer from the context.\n",
    "This means that your question MUST NOT mention something like \"according to the passage/map/picture/graph/chart/line/context\".\n",
    "In your question you should ask to make a comparison between data in the context between var_1 and var_2 provided. \n",
    "You should enrich your question with the costants and the vars provided if they are not None.\n",
    "All words of both the answer and the question must be in english.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Question: (your question)\n",
    "Answer: (your answer to the question)\n",
    "\n",
    "Here is the context, var1, var2, constant1, constant2.\n",
    "\n",
    "Context: {context}\\n\n",
    "Var1: {var1}\\n\n",
    "Var2: {var2}\\n\n",
    "Constant1: {constant1}\\n\n",
    "Constant2: {constant2}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate first part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "N_GENERATIONS = 15\n",
    "outputs = []\n",
    "\n",
    "for doc in tqdm(random.sample(docs, N_GENERATIONS)):\n",
    "    context = doc.page_content\n",
    "    keywords = json.loads(\"{\" + doc.metadata[\"keywords\"] + \"}\")\n",
    "    country = keywords[\"country\"]\n",
    "    year = keywords[\"year\"]\n",
    "    target = keywords[\"target\"] if \"target\" in keywords else None\n",
    "\n",
    "    output_QA = llm.invoke(QA_generation_prompt_1.format(context = context, \n",
    "                                                       country = country, \n",
    "                                                       year = year, \n",
    "                                                       target = target)).content\n",
    "    question = output_QA.split(\"Question: \")[-1].split(\"Answer: \")[0]\n",
    "    answer = output_QA.split(\"Answer: \")[-1]\n",
    "    outputs.append(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"source_doc\": doc.metadata[\"source\"],\n",
    "        }\n",
    "    )\n",
    "    sleep(5)\n",
    "\n",
    "with open(\"../evaluation/dataset/all_QA_with_images_2.json\", \"w\") as f:\n",
    "    json.dump(outputs, f, indent = 4, ensure_ascii = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate second part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from itertools import product\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def random_matcher(const_list1, const_list2, var_list, n_iter):\n",
    "\n",
    "    const_list_prod = [(i1, i2) for i1, i2 in product(const_list1[\"values\"], const_list2[\"values\"]) if i1 != i2] if const_list2 else const_list1[\"values\"]\n",
    "    prod_list = list(product(const_list_prod, [(i1, i2) for i1, i2 in product(var_list[\"values\"], var_list[\"values\"]) if i1 != i2]))\n",
    "    if const_list2 == None:\n",
    "        prod_list = [(tuple[0], tuple[1][0], tuple[1][1]) for tuple in prod_list]\n",
    "    else:\n",
    "        prod_list = [(tuple[0][0], tuple[0][1], tuple[1][0], tuple[1][1]) for tuple in prod_list]\n",
    "\n",
    "    return [random_tuple for random_tuple in random.sample(prod_list, n_iter)]\n",
    "\n",
    "\n",
    "def generate_qa(docs, var1, var2, const1, const2):\n",
    "    context = \" \".join([doc.page_content for doc in docs])\n",
    "    output_QA = llm.invoke(QA_generation_prompt_2\n",
    "            .format(context = context, \n",
    "                    var1 = var1, \n",
    "                    var2 = var2, \n",
    "                    constant1 = const1, \n",
    "                    constant2 = const2)).content\n",
    "    if output_QA == \"idk.\":\n",
    "        return None\n",
    "    question = output_QA.split(\"Question: \")[-1].split(\"Answer: \")[0]\n",
    "    answer = output_QA.split(\"Answer: \")[-1]\n",
    "    sleep(5)\n",
    "    \n",
    "    if question == \"idk.\" or answer == \"idk.\":\n",
    "        return None\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"source_doc\": [doc.metadata[\"source\"] for doc in docs],\n",
    "    }\n",
    "    \n",
    "\n",
    "def save_dataset(filename, data):\n",
    "    path = \"../evaluation/dataset\"\n",
    "    with open(f\"{path}/{filename}\", \"w\") as f:\n",
    "        json.dump(data, f, indent = 4, ensure_ascii = False)\n",
    "\n",
    "\n",
    "years = {\"filterable\": True, \"values\": [2015, 2017, 2019, 2021, 2022, 2023, 2024]}\n",
    "countries = {\"filterable\": False, \"values\": [\"France\", \"Spain\", \"Germany\", \"UK\", \"Italy\", \"Brazil\"]}\n",
    "brands = {\"filterable\": False, \"values\": [\"3shape\", \"Dentsply Sirona\", \"Ivoclair\", \"Bego\", \"3M\", \"Ines Icore\", \"Caresstream\", \"Amann Girbach\"]}\n",
    "products = {\"filterable\": False, \"values\": [\"Intraoral scanner\", \"3d printer\"]}\n",
    "\n",
    "all_QA = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# year -> country\n",
    "qa1 = []\n",
    "tuples = random_matcher(years, None, countries, n_iter = 100)\n",
    "for tuple in tqdm(tuples):\n",
    "    (year, country1, country2) = tuple[0], tuple[1], tuple[2]\n",
    "    docs1 = vector_store.similarity_search(f\"{country1}\", k = 4, filter = lambda doc: f\"{year}\" in doc.metadata[\"keywords\"])\n",
    "    docs2 = vector_store.similarity_search(f\"{country2}\", k = 4, filter = lambda doc: f\"{year}\" in doc.metadata[\"keywords\"])\n",
    "    qa = generate_qa(docs1 + docs2, var1 = country1, var2 = country2, const1 = year, const2 = None)\n",
    "    if qa != None:\n",
    "        qa1.append(qa)\n",
    "\n",
    "save_dataset(\"all_QA_years_countries.json\", qa1)\n",
    "\n",
    "#(brand, country) -> year\n",
    "qa2 = []\n",
    "tuples = random_matcher(brands, countries, years, n_iter = 10)\n",
    "for tuple in tqdm(tuples):\n",
    "    (brand, country, year1, year2) = tuple[0], tuple[1], tuple[2], tuple[3]\n",
    "    docs1 = vector_store.similarity_search(f\"{country}, {brand}\", k = 4, filter = lambda doc: f\"{year1}\" in doc.metadata[\"keywords\"])\n",
    "    docs2 = vector_store.similarity_search(f\"{country}, {brand}\", k = 4, filter = lambda doc: f\"{year2}\" in doc.metadata[\"keywords\"])\n",
    "    qa = generate_qa(docs1 + docs2, var1 = year1, var2 = year2, const1 = country, const2 = brand)\n",
    "    if qa != None:\n",
    "        qa2.append(qa)\n",
    "\n",
    "save_dataset(\"all_QA_brands&countries_years.json\", qa2)\n",
    "\n",
    "#(country, year) -> brand\n",
    "qa3 = []\n",
    "tuples = random_matcher(countries, years, brands, n_iter = 100)\n",
    "for tuple in tqdm(tuples):\n",
    "    (country, year, brand1, brand2) = tuple[0], tuple[1], tuple[2], tuple[3]\n",
    "    docs1 = vector_store.similarity_search(f\"{country}, {brand1}\", k = 4, filter = lambda doc: f\"{year}\" in doc.metadata[\"keywords\"])\n",
    "    docs2 = vector_store.similarity_search(f\"{country}, {brand2}\", k = 4, filter = lambda doc: f\"{year}\" in doc.metadata[\"keywords\"])\n",
    "    qa = generate_qa(docs1 + docs2, var1 = brand1, var2 = brand2, const1 = country, const2 = year)\n",
    "    if qa != None:\n",
    "        qa3.append(qa)\n",
    "\n",
    "save_dataset(\"all_QA_countries&years_brands.json\", qa3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
