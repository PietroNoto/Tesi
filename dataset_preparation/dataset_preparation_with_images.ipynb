{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents into pages, with text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        files.append(complete_path)\n",
    "\n",
    "docs = []\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    async for doc in loader.alazy_load():\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove first pages and index pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc for doc in docs if doc.metadata[\"page\"] != 0]\n",
    "\n",
    "docs = [doc for doc in docs \n",
    "        if not doc.page_content.lower().startswith((\"index\", \"table of contents\", \"índice\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate text with image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"../indexing/image_descriptions/image_descriptions.json\", \"r\") as f:\n",
    "    image_descriptions = json.load(f)\n",
    "\n",
    "    for imd in image_descriptions:\n",
    "        file = f\"../sources/{imd[\"file\"]}.pdf\"\n",
    "        page = imd[\"page\"]\n",
    "        doc = next(filter(lambda doc: doc.metadata[\"source\"] == file and doc.metadata[\"page\"] == page, docs), None)\n",
    "        if doc != None:\n",
    "            doc.page_content += f\"\\n{imd[\"image_description\"]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()\n",
    "\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        if \"non-en\" not in p.metadata[\"keywords\"]:\n",
    "            p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•▪\\-*✔➢●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())\n",
    "\n",
    "\n",
    "remove_non_ASCII(docs)\n",
    "decapitalize_content(docs)\n",
    "remove_bullets(docs)\n",
    "remove_escape(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_and_split(docs: list[Document], splitter):\n",
    "\n",
    "    from collections import defaultdict\n",
    "    \n",
    "\n",
    "    docs_groups = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        docs_groups[doc.metadata[\"source\"]].append(doc)\n",
    "\n",
    "    giant_docs = []\n",
    "    for _, docs in docs_groups.items():\n",
    "        giant_doc = {}\n",
    "        metadata = {k: v for k, v in docs[0].metadata.items() if k != \"page\"}\n",
    "        page_content = \"\"\n",
    "        for doc in docs:\n",
    "            page_content += doc.page_content\n",
    "        giant_doc[\"metadata\"] = metadata\n",
    "        giant_doc[\"page_content\"] = page_content\n",
    "        giant_docs.append(giant_doc)\n",
    "\n",
    "    files = []\n",
    "    for gdoc in giant_docs:\n",
    "        page_contents = splitter.split_text(gdoc[\"page_content\"])\n",
    "        files += [{\"metadata\": gdoc[\"metadata\"], \"page_content\": pc} for pc in page_contents]\n",
    "\n",
    "    files = [Document(metadata = file[\"metadata\"], page_content = file[\"page_content\"]) for file in files]\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def save_chunks(pages: list, path: str):\n",
    "\n",
    "    from langchain_core.load import dumpd\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for chunk in range(len(pages)):\n",
    "        full_path = path + \"/\" + \"chunk_\" + str(chunk + 1)\n",
    "        with open(full_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)\n",
    "\n",
    "\n",
    "chunk_size = 2_000\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_size/10,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "docs = merge_and_split(docs, text_splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup agent for QA generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o\",\n",
    "    max_tokens = 256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a question and an answer given a context, country, year and, optionally, a target, which can be clinic (dental practise) and/or lab (laboratory).\n",
    "Your question should be answerable with a one-sentenced answer from the context.\n",
    "Your question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "Your question should be enriched with the country and year provided, just like \"What is the most quoted brand in France in 2018?\". \n",
    "Additionally, if the target is either clinic or lab (but not both) you should mention it in your question.\n",
    "All words of both the answer and the question must be in english.\n",
    "When your question asks for the best item out of an ensemble, instead of picking just the best, try to include a leaderboard of at least three items in your answer.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Question: (your question)\n",
    "Answer: (your answer to the question)\n",
    "\n",
    "Here is the context, country, year and target.\n",
    "\n",
    "Context: {context}\\n\n",
    "Country: {country}\\n\n",
    "Year: {year}\\n\n",
    "Target: {target}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "N_GENERATIONS = 100\n",
    "outputs = []\n",
    "\n",
    "for doc in tqdm(random.sample(docs, N_GENERATIONS)):\n",
    "    context = doc.page_content\n",
    "    keywords = json.loads(\"{\" + doc.metadata[\"keywords\"] + \"}\")\n",
    "    country = keywords[\"country\"]\n",
    "    year = keywords[\"year\"]\n",
    "    target = keywords[\"target\"] if \"target\" in keywords else None\n",
    "\n",
    "    output_QA = llm.invoke(QA_generation_prompt.format(context = context, \n",
    "                                                       country = country, \n",
    "                                                       year = year, \n",
    "                                                       target = target)).content\n",
    "    question = output_QA.split(\"Question: \")[-1].split(\"Answer: \")[0]\n",
    "    answer = output_QA.split(\"Answer: \")[-1]\n",
    "    outputs.append(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"source_doc\": doc.metadata[\"source\"],\n",
    "        }\n",
    "    )\n",
    "    sleep(5)\n",
    "\n",
    "with open(\"../evaluation/dataset/all_QA_with_images_2.json\", \"w\") as f:\n",
    "    json.dump(outputs, f, indent = 4, ensure_ascii = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = {\n",
    "    \"../sources/BEQ_2301_OVERALL_multi.pdf\": \"\\\"target\\\": \\\"clinic\\\"\",\n",
    "    \"../sources/OMNI_DIGITAL_EU_15_CLI_.pdf\": \"\\\"target\\\": \\\"clinic\\\"\",\n",
    "    \"../sources/OMNI_DIGITAL_EU_15_CLI_LAB_Executive_Summary_.pdf\": \"\\\"target\\\": \\\"all\\\"\",\n",
    "    \"../sources/OMNI_DIGITAL_EU_15_LAB_.pdf\": \"\\\"target\\\": \\\"lab\\\"\",\n",
    "    \"../sources/OMNI_DIGITAL_EU_21_CLI_LAB_INTEGRATED_.pdf\": \"\\\"target\\\": \\\"all\\\"\",\n",
    "    \"../sources/OMNI_DIGITAL_ITA_19_CLI_LAB_INTEGRATED_.pdf\": \"\\\"target\\\": \\\"all\\\"\",\n",
    "    \"../sources/OMNI_DIGITAL_SPA_19_CLI_.pdf\": \"\\\"target\\\": \\\"clinic\\\"\",\n",
    "    \"../sources/OMNI_DIGITAL_SPA_19_CLI_LAB_INTEGRATED_spagnolo.pdf\": \"\\\"target\\\": \\\"all\\\"\",\n",
    "    \"../sources/OMNI_DIGITAL_SPA_19_LAB_.pdf\": \"\\\"target\\\": \\\"lab\\\"\",\n",
    "    \"../sources/OMNI-DIGITAL_ITA_17_CLI_.pdf\": \"\\\"target\\\": \\\"clinic\\\"\",\n",
    "    \"../sources/OMNI-OMNI-DIGITAL_ITA_23_CLI_.pdf\": \"\\\"target\\\": \\\"clinic\\\"\",\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "for model in [\"minilm_l6\", \"minilm_l12\", \"mpnet_base_v2\", \"multilingual\", \"text_embedding_3_large\"]:\n",
    "    for chunk_type in [\"page_chunking\", \"fixed_number\"]:\n",
    "        for chunk_size in [256, 384]:\n",
    "            for chunk_overlap in [0, 20, 50, 100]:\n",
    "\n",
    "                if chunk_type == \"fixed_number\":\n",
    "                    chunking = f\"{chunk_size}_{chunk_overlap}\"\n",
    "                else:\n",
    "                    chunking = chunk_type\n",
    "\n",
    "                path = f\"../indexing/models/Text+Images/{model}/{chunking}/{chunking}_{model}\"\n",
    "                with open(path, \"r\") as fr:\n",
    "                    vstore = json.load(fr)\n",
    "\n",
    "                    for id in vstore:\n",
    "                        for edit in edits:\n",
    "                            if vstore[id][\"metadata\"][\"source\"] == edit:\n",
    "                                vstore[id][\"metadata\"][\"keywords\"] += f\",{edits[edit]}\"\n",
    "                                break\n",
    "\n",
    "                with open(path, \"w\") as fw:\n",
    "                    json.dump(vstore, fw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../indexing/models/Text+Images/minilm_l12/384_100/384_100_minilm_l12\"\n",
    "with open(path, \"r\") as f:\n",
    "    vstore = json.load(f)\n",
    "\n",
    "    for id in vstore:\n",
    "        print(vstore[id][\"metadata\"][\"keywords\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
