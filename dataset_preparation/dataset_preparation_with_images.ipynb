{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split documents into pages, with text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "\n",
    "\n",
    "folder = \"../sources\"\n",
    "files = []\n",
    "\n",
    "for fname in os.listdir(folder):\n",
    "    complete_path = os.path.join(folder, fname)\n",
    "    if os.path.isfile(complete_path):\n",
    "        files.append(complete_path)\n",
    "\n",
    "docs = []\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(file)\n",
    "    async for doc in loader.alazy_load():\n",
    "        docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove first pages and index pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [doc for doc in docs if doc.metadata[\"page\"] != 0]\n",
    "\n",
    "docs = [doc for doc in docs \n",
    "        if not doc.page_content.lower().startswith((\"index\", \"table of contents\", \"índice\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate text with image descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"../indexing/image_descriptions/image_descriptions.json\", \"r\") as f:\n",
    "    image_descriptions = json.load(f)\n",
    "\n",
    "    for imd in image_descriptions:\n",
    "        file = f\"../sources/{imd[\"file\"]}.pdf\"\n",
    "        page = imd[\"page\"]\n",
    "        doc = next(filter(lambda doc: doc.metadata[\"source\"] == file and doc.metadata[\"page\"] == page, docs), None)\n",
    "        if doc != None:\n",
    "            doc.page_content += f\"\\n{imd[\"image_description\"]}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def decapitalize_content(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns document content into lower case\"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = p.page_content.lower()\n",
    "\n",
    "\n",
    "def remove_non_ASCII(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes non ASCII characters from document. Not suitable for many non english languages \n",
    "    which have several non ASCII characters \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        if \"non-en\" not in p.metadata[\"keywords\"]:\n",
    "            p.page_content = re.sub(r\"[^\\x00-\\x7F]+\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_bullets(pages: list[str]):\n",
    "\n",
    "    \"\"\"Removes bullets from document \"\"\"\n",
    "\n",
    "    for p in pages:\n",
    "        p.page_content = re.sub(r\"^[→•▪\\-*✔➢●✗]\\s*\", \"\", p.page_content, flags = re.MULTILINE)\n",
    "        p.page_content = re.sub(r\"\\d+\\.(?=\\s*[a-zA-Z])\", \"\", p.page_content)\n",
    "\n",
    "\n",
    "def remove_escape(pages: list[str]):\n",
    "\n",
    "    \"\"\"Turns multiple consecutive escape characters into a single white space\"\"\"\n",
    "    \n",
    "    for p in pages:\n",
    "        p.page_content = ' '.join(p.page_content.split())\n",
    "\n",
    "\n",
    "remove_non_ASCII(docs)\n",
    "decapitalize_content(docs)\n",
    "remove_bullets(docs)\n",
    "remove_escape(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "\n",
    "def merge_and_split(docs: list[Document], splitter):\n",
    "\n",
    "    from collections import defaultdict\n",
    "    \n",
    "\n",
    "    docs_groups = defaultdict(list)\n",
    "    for doc in docs:\n",
    "        docs_groups[doc.metadata[\"source\"]].append(doc)\n",
    "\n",
    "    giant_docs = []\n",
    "    for _, docs in docs_groups.items():\n",
    "        giant_doc = {}\n",
    "        metadata = {k: v for k, v in docs[0].metadata.items() if k != \"page\"}\n",
    "        page_content = \"\"\n",
    "        for doc in docs:\n",
    "            page_content += doc.page_content\n",
    "        giant_doc[\"metadata\"] = metadata\n",
    "        giant_doc[\"page_content\"] = page_content\n",
    "        giant_docs.append(giant_doc)\n",
    "\n",
    "    files = []\n",
    "    for gdoc in giant_docs:\n",
    "        page_contents = splitter.split_text(gdoc[\"page_content\"])\n",
    "        files += [{\"metadata\": gdoc[\"metadata\"], \"page_content\": pc} for pc in page_contents]\n",
    "\n",
    "    files = [Document(metadata = file[\"metadata\"], page_content = file[\"page_content\"]) for file in files]\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def save_chunks(pages: list, path: str):\n",
    "\n",
    "    from langchain_core.load import dumpd\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for chunk in range(len(pages)):\n",
    "        full_path = path + \"/\" + \"chunk_\" + str(chunk + 1)\n",
    "        with open(full_path, \"w\") as ser_file:\n",
    "            page_d = dumpd(pages[chunk])\n",
    "            json.dump(page_d, ser_file)\n",
    "\n",
    "\n",
    "chunk_size = 5_000\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap = chunk_size/10,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "docs = merge_and_split(docs, text_splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup agent for QA generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = \"https://keystone1.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\",\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"],\n",
    "    api_version = \"2024-08-01-preview\",\n",
    "    azure_deployment = \"gpt-4o\",\n",
    "    max_tokens = 256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a question and an answer given a context, country, year and, optionally, a target, which can be clinic (dental practise) and/or lab (laboratory).\n",
    "Your question should be and answerable with a one-sentenced factual answer from the context.\n",
    "This means that your question MUST NOT mention something like \"according to the passage/map/picture/graph/chart/line\" or \"context\".\n",
    "Identify, if any, a country from the context and refer to it when you formulate the question, otherwise use the country provided.\n",
    "Alongside the country, enrich the question with the year provided, for example: \"Which is the top brand in France in 2023?\"\n",
    "Additionally, if the target is either clinic or lab (but not both) you should mention it in your question.\n",
    "All words of both the answer and the question must be in english.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Question: (your question)\n",
    "Answer: (your answer to the question)\n",
    "\n",
    "Here is the context, country, year and target.\n",
    "\n",
    "Context: {context}\\n\n",
    "Country: {country}\\n\n",
    "Year: {year}\\n\n",
    "Target: {target}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "N_GENERATIONS = 15\n",
    "outputs = []\n",
    "\n",
    "for doc in tqdm(random.sample(docs, N_GENERATIONS)):\n",
    "    context = doc.page_content\n",
    "    keywords = json.loads(\"{\" + doc.metadata[\"keywords\"] + \"}\")\n",
    "    country = keywords[\"country\"]\n",
    "    year = keywords[\"year\"]\n",
    "    target = keywords[\"target\"] if \"target\" in keywords else None\n",
    "\n",
    "    output_QA = llm.invoke(QA_generation_prompt.format(context = context, \n",
    "                                                       country = country, \n",
    "                                                       year = year, \n",
    "                                                       target = target)).content\n",
    "    question = output_QA.split(\"Question: \")[-1].split(\"Answer: \")[0]\n",
    "    answer = output_QA.split(\"Answer: \")[-1]\n",
    "    outputs.append(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"source_doc\": doc.metadata[\"source\"],\n",
    "        }\n",
    "    )\n",
    "    sleep(5)\n",
    "\n",
    "with open(\"../evaluation/dataset/all_QA_with_images_2.json\", \"w\") as f:\n",
    "    json.dump(outputs, f, indent = 4, ensure_ascii = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
